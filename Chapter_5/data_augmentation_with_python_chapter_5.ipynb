{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/duchaba/Data-Augmentation-with-Python/blob/main/data_augmentation_with_python_chapter_5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üåª Welcome to Chapter 5, Text Augmentation\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "I am glad to see you using this Python Notebook. üê∂\n",
        "\n",
        "The Python Notebook is an integral part of the book. You can add new ‚Äúcode cells‚Äù to extend the functions, add your data, and explore new possibilities, such as downloading additional real-world datasets from the Kaggle website and coding the **Fun challenges**. Furthermore, the book has **Fun facts**, in-depth discussion about augmentation techniques, and Pluto, an imaginary Siberian Huskey coding companion. Together they will guide you every steps of the way.\n",
        "\n",
        "Pluto encourages you to copy or save a copy of this Python Notebook to your local space and add the ‚Äútext cells‚Äù to keep your notes. In other words, read the book and copy the relevant concept to this Python Notebook‚Äôs text-cells. Thus, you can have the explanation, note, original code, your code, and any crazy future ideas in one place.  \n",
        "\n",
        "\n",
        "üíó I hope you enjoy reading the book and hacking code as much as I enjoy writing it.\n",
        "\n",
        "\n",
        "## üåü Amazon Book\n",
        "\n",
        "---\n",
        "\n",
        "- The book is available on the Amazon Book website:\n",
        "  - https://www.amazon.com/dp/1803246456\n",
        "\n",
        "  - Author: Duc Haba\n",
        "  - Published: 2023\n",
        "  - Page count: 390+\n",
        "\n",
        "\n",
        "- The original Python Notebook is on:\n",
        "  - https://github.com/PacktPublishing/Data-Augmentation-with-Python/blob/main/Chapter_5/data_augmentation_with_python_chapter_5.ipynb\n",
        "\n",
        "- üöÄ Click on the blue \"Open in Colab\" button at the top of this page to begin hacking.\n",
        "\n"
      ],
      "metadata": {
        "id": "rad13-eq4fAm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üòÄ Excerpt from Chapter 5, Text Augmentation\n",
        "\n",
        "---\n",
        "\n",
        "> In case you haven‚Äôt bought the book. Here is an teaser from the first page of Chapter 5.\n",
        "\n",
        "---\n",
        "\n",
        "Text augmentation is a technique that is used in Natural Language Processing (NLP) to generate additional data by modifying or creating new text from existing text data. Text augmentation involves techniques such as character swapping, noise injection, synonym replacement, word deletion, word insertion, and word swapping. Image and text augmentation have the same goal. They strive to increase the size of the training dataset and improve AI prediction accuracy.\n",
        "\n",
        "Text augmentation is relatively more challenging to evaluate because it is not as intuitive as image augmentation. The intent of an image augmentation technique is clear, such as flipping a photo, but a character-swapping technique will be disorienting to the reader. Therefore, readers might perceive the benefits as subjective.\n",
        "\n",
        "The effectiveness of text augmentation depends on the quality of the generated data and the specific NLP task being performed. It can be challenging to determine the appropriate safe level of text augmentation that is required for a given dataset, and it often requires experimentation and testing to achieve the desired results.\n",
        "\n",
        "Customer feedback or social media chatter is fair game for text augmentation because the writing is messy and, predominantly, contains grammatical errors. Conversely, legal documents or written medical communications, such as doctor‚Äôs prescriptions or reports, are off-limits because the message is precise. In other words, error injections, synonyms, or even AI-generated text might change the legal or medical meaning beyond the safe level.\n",
        "\n",
        "The biases in text augmentation are equally difficult to discern. For example, adding noise by purposing misspell words using the Keyboard augmentation method might introduce bias against real-world tweets, which typically contain misspelled words. There are no generalized rules to follow, and the answer only becomes evident after thoroughly studying the data and reviewing the AI forecasting objective.\n",
        "\n",
        "---\n",
        "\n",
        "Fun fact\n",
        "\n",
        "---\n",
        "\n",
        "As generative AI becomes more widely available, you can use OpenAI‚Äôs GPT-3, Google Bard or Facebook‚Äôs Roberta system to generate original articles for text augmentation. For example, you can ask generative AI to create positive or negative reviews about a company product, then use the AI-written articles to train predictive AI on sentiment analysis.\n",
        "\n",
        "---\n",
        "\n",
        "In Chapter 5, you will learn about text augmentation and how to code the methods in Python notebooks. In particular, we will cover the following topics:\n",
        "\n",
        "- Character augmenting\n",
        "\n",
        "- Word augmenting\n",
        "\n",
        "- Sentence and flow augmenting\n",
        "\n",
        "- Text augmentation libraries\n",
        "\n",
        "- Real-world text datasets\n",
        "\n",
        "- Reinforce learning through Python Notebook\n",
        "\n",
        "Let‚Äôs get started with the simplest topic, character augmentation.\n",
        "\n",
        "---\n",
        "\n",
        "üå¥ *end of excerpt from the book*"
      ],
      "metadata": {
        "id": "ZZ8-JIXv6jmV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GitHub Clone"
      ],
      "metadata": {
        "id": "4b52gSlp60SN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# git version should be 2.17.1 or higher\n",
        "!git --version"
      ],
      "metadata": {
        "id": "6oeDAu1u6zWf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f9cfe4a0-0900-480c-a004-24d28c01885f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "git version 2.34.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "url = 'https://github.com/PacktPublishing/Data-Augmentation-with-Python'\n",
        "!git clone {url}"
      ],
      "metadata": {
        "id": "J9buwUR767Te",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0065c8fb-c93e-4e66-c8a8-3262b550ea44"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Data-Augmentation-with-Python'...\n",
            "remote: Enumerating objects: 462, done.\u001b[K\n",
            "remote: Counting objects: 100% (440/440), done.\u001b[K\n",
            "remote: Compressing objects: 100% (261/261), done.\u001b[K\n",
            "remote: Total 462 (delta 240), reused 359 (delta 178), pack-reused 22\u001b[K\n",
            "Receiving objects: 100% (462/462), 139.70 MiB | 15.58 MiB/s, done.\n",
            "Resolving deltas: 100% (240/240), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fetch file from URL (Optional)\n",
        "\n",
        "- Uncommend the below 2 code cells if you want to use URL and not Git Clone"
      ],
      "metadata": {
        "id": "lOawA01L7Ok0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import requests\n",
        "# #\n",
        "# def fetch_file(url, dst):\n",
        "#   downloaded_obj = requests.get(url)\n",
        "#   with open(dst, \"wb\") as file:\n",
        "#     file.write(downloaded_obj.content)\n",
        "#   return"
      ],
      "metadata": {
        "id": "HsmQ7rgj67Ww"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# url = ''\n",
        "# dst = 'pluto_chapter_1.py'\n",
        "# fetch_file(url,dst)"
      ],
      "metadata": {
        "id": "_nKp0nxT67aH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run Pluto\n",
        "\n",
        "- Instantiate up Pluto, aka. \"Pluto, wake up!\""
      ],
      "metadata": {
        "id": "6462KTtr7cFQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# %% CARRY-OVER code install\n",
        "\n",
        "!pip install opendatasets --upgrade\n",
        "!pip install pyspellchecker"
      ],
      "metadata": {
        "id": "jCGiTw16ZxVH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0be0328-68a1-4d58-f4de-68062e4f42b3"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting opendatasets\n",
            "  Downloading opendatasets-0.1.22-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from opendatasets) (4.66.1)\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.10/dist-packages (from opendatasets) (1.5.16)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from opendatasets) (8.1.7)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (1.16.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (2023.7.22)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (2.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (2.31.0)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (8.0.1)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (2.0.4)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (6.0.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->kaggle->opendatasets) (0.5.1)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.10/dist-packages (from python-slugify->kaggle->opendatasets) (1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle->opendatasets) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle->opendatasets) (3.4)\n",
            "Installing collected packages: opendatasets\n",
            "Successfully installed opendatasets-0.1.22\n",
            "Collecting pyspellchecker\n",
            "  Downloading pyspellchecker-0.7.2-py3-none-any.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m34.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyspellchecker\n",
            "Successfully installed pyspellchecker-0.7.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#load and run the pluto chapter 1 Python code.\n",
        "pluto_file = 'Data-Augmentation-with-Python/pluto/pluto_chapter_2.py'\n",
        "%run {pluto_file}"
      ],
      "metadata": {
        "id": "u3zbkOO86_WB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23b8e897-236d-498b-8169-0cf2d0320bad"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------------------- : ----------------------------\n",
            "            Hello from class : <class '__main__.PacktDataAug'> Class: PacktDataAug\n",
            "                   Code name : Pluto\n",
            "                   Author is : Duc Haba\n",
            "---------------------------- : ----------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# check out the AI auto generated doc.\n",
        "help(pluto)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LQufTMVdQVGP",
        "outputId": "7b59c321-3c99-44eb-e359-b32c99f521f7"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Help on PacktDataAug in module __main__ object:\n",
            "\n",
            "class PacktDataAug(builtins.object)\n",
            " |  PacktDataAug(name='Pluto', is_verbose=True, *args, **kwargs)\n",
            " |  \n",
            " |  The PacktDataAug class is the based class for the\n",
            " |  \"Data Augmentation with Python\" book.\n",
            " |  \n",
            " |  Methods defined here:\n",
            " |  \n",
            " |  __init__(self, name='Pluto', is_verbose=True, *args, **kwargs)\n",
            " |      This is the constructor function.\n",
            " |      \n",
            " |      Args:\n",
            " |      \n",
            " |       name (str): It requires a name for the object. The default is 'Pluto'\n",
            " |       verbose (bool):  The default value of `verbose` is True. This function prints out the\n",
            " |          name of the object if `is_verbose == True`. This is used to debug\n",
            " |          code. When you are ready to deploy the model, then you should set\n",
            " |          `is_verbose == False` in order to avoid printing out diagnostic\n",
            " |          messages.\n",
            " |      \n",
            " |        Additionally, this function takes any number of other\n",
            " |        parameters. These parameters are stored in `**kwargs` and are\n",
            " |        accessed via the function `get_kwargs()`. See the documentation\n",
            " |        for `get_kwargs()` for more details.\n",
            " |        Note that `__init__()` is\n",
            " |        automatically called when you create a new object.\n",
            " |      \n",
            " |      Returns:\n",
            " |        None.\n",
            " |  \n",
            " |  build_sf_fname(self, df)\n",
            " |      This method builds the file name for a given row in the State Farm DataFrame.\n",
            " |      \n",
            " |      Args:\n",
            " |        df (Pandas DataFrame): The State Farm DataFrame.\n",
            " |      \n",
            " |      Returns:\n",
            " |        None\n",
            " |  \n",
            " |  build_shoe_fname(self, start_path)\n",
            " |      This method builds the file name for a given directory.\n",
            " |      \n",
            " |      Args:\n",
            " |        start_path (str): The starting directory.\n",
            " |      \n",
            " |      Returns:\n",
            " |        DataFrame: The DataFrame containing the file names.\n",
            " |  \n",
            " |  check_spelling(self, df, col_dest='description')\n",
            " |      This method checks the spelling in a column and returns a new column \n",
            " |      \"misspelled\" and \"misspelled_count\".\n",
            " |      \n",
            " |      Args:\n",
            " |        df (DataFrame): The input DataFrame.\n",
            " |        col_dest (str): The column name to be checked, default \"description\"\n",
            " |      \n",
            " |      Returns:\n",
            " |        None\n",
            " |  \n",
            " |  count_word(self, df, col_dest='description')\n",
            " |      This method counts the number of words in a column named \"wordc\"\n",
            " |      \n",
            " |      Args:\n",
            " |        df (DataFrame): The input DataFrame.\n",
            " |        col_dest (str): The column name to be counted, default \"description\"\n",
            " |      \n",
            " |      Returns:\n",
            " |        None\n",
            " |  \n",
            " |  draw_batch(self, df_filenames, disp_max=10, is_shuffle=False, figsize=(16, 8))\n",
            " |      This method draws the images specified in the DataFrame.\n",
            " |      \n",
            " |      Args:\n",
            " |        df_filenames (Pandas DataFrame): The DataFrame containing the file names.\n",
            " |        disp_max (int): The maximum number of images to be drawn. Default is 10.\n",
            " |        is_shuffle (bool): Whether to shuffle the images. Default is False.\n",
            " |        figsize (tuple): The figure size. Default is (16,8).\n",
            " |      \n",
            " |      Returns:\n",
            " |        None\n",
            " |  \n",
            " |  draw_word_count(self, df, wc='wordc', is_stack_verticle=True)\n",
            " |      This method creates two plots:\n",
            " |        1. a boxplot of word count\n",
            " |        2. a histogram of word count\n",
            " |      \n",
            " |      Args:\n",
            " |        df (DataFrame): The input DataFrame.\n",
            " |        wc (str): The column name of word count, default \"wordc\".\n",
            " |        is_stack_verticle (bool): Whether to stack the two plots vertically. Default is True.\n",
            " |      \n",
            " |      Returns:\n",
            " |        None\n",
            " |  \n",
            " |  fetch_df(self, csv, sep=',')\n",
            " |      This method reads and loads a CSV file into a Pandas DataFrame.\n",
            " |      \n",
            " |      Args:\n",
            " |        csv (str): The path to a CSV file.\n",
            " |        sep (str): The column separator character, default is comma \",\".\n",
            " |      \n",
            " |      Returns:\n",
            " |        DataFrame: A pandas DataFrame.\n",
            " |  \n",
            " |  fetch_kaggle_comp_data(self, cname)\n",
            " |      This method downloads and unzip the data from the Competition on Kaggle.\n",
            " |      You need to join the competition before downloading the data.\n",
            " |      \n",
            " |      Args:\n",
            " |        cname (str): The name of the competition on Kaggle.\n",
            " |      \n",
            " |      Returns:\n",
            " |        None\n",
            " |  \n",
            " |  fetch_kaggle_dataset(self, url, dest='kaggle')\n",
            " |      This method downloads the data from the Kaggle's dataset.\n",
            " |      You need NOT to join the competition before downloading the data.\n",
            " |      \n",
            " |      Args:\n",
            " |        url (str): The url of the dataset on Kaggle.\n",
            " |        dest (str): the destination path, default is \"kaggle\" directory.\n",
            " |      \n",
            " |      Returns:\n",
            " |        None\n",
            " |  \n",
            " |  make_dir_dataframe(self, start_path)\n",
            " |      This method builds the file name for a given directory.\n",
            " |      \n",
            " |      Args:\n",
            " |        start_path (str): The starting directory.\n",
            " |      \n",
            " |      Returns:\n",
            " |        DataFrame: The DataFrame containing the file names.\n",
            " |  \n",
            " |  print_batch_text(self, df_orig, disp_max=6, cols=['title', 'description'], is_larger_font=True)\n",
            " |      This method shows a batch of text data.\n",
            " |      \n",
            " |      Args:\n",
            " |        df_orig (DataFrame): The input DataFrame.\n",
            " |        disp_max (int): The maximum number of rows to be displayed. Default is 6.\n",
            " |        cols (list): The list of columns to display. Default is [\"title\", \"description\"].\n",
            " |        is_larger_font (bool): Whether to use larger font. Default is True.\n",
            " |      \n",
            " |      Returns:\n",
            " |        None\n",
            " |  \n",
            " |  remember_kaggle_access_keys(self, username, key)\n",
            " |      This method takes a username and a Kaggle API key as arguments and stores\n",
            " |      them in the class object.\n",
            " |      \n",
            " |      Args:\n",
            " |       username (str): The Kaggle username.\n",
            " |       key (str): The Kaggle API key.\n",
            " |      \n",
            " |      Returns:\n",
            " |       None\n",
            " |  \n",
            " |  say_sys_info(self)\n",
            " |      Print out system information. Useful for\n",
            " |      debugging purposes. Prints out information such as\n",
            " |      the system time, platform, Python version, PyTorch\n",
            " |      version, Pandas version, PIL version, and\n",
            " |      Matplotlib version. Also prints the number of CPU\n",
            " |      cores and the CPU speed.\n",
            " |      \n",
            " |      Note that this function is added to the class `PacktDataAug` via\n",
            " |      the decorator `@add_method()`. This means that you can\n",
            " |      call this function as `p.say_system_info()`,\n",
            " |      where `p` is an instance of `PacktDAtaAug`.\n",
            " |      \n",
            " |      Args:\n",
            " |        None\n",
            " |      \n",
            " |      Returns:\n",
            " |        None\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors defined here:\n",
            " |  \n",
            " |  __dict__\n",
            " |      dictionary for instance variables (if defined)\n",
            " |  \n",
            " |  __weakref__\n",
            " |      list of weak references to the object (if defined)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Verify Pluto"
      ],
      "metadata": {
        "id": "yhRR0JSf746h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pluto.say_sys_info()"
      ],
      "metadata": {
        "id": "nBIw7fiI6_Zy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48a16778-f319-4bc5-8223-f02a16fe8337"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------------------- : ----------------------------\n",
            "                 System time : 2023/09/15 23:51\n",
            "                    Platform : linux\n",
            "     Pluto Version (Chapter) : 2.0\n",
            "             Python (3.7.10) : actual: 3.10.12 (main, Jun 11 2023, 05:26:28) [GCC 11.4.0]\n",
            "            PyTorch (1.11.0) : actual: 2.0.1+cu118\n",
            "              Pandas (1.3.5) : actual: 1.5.3\n",
            "                 PIL (9.0.0) : actual: 9.4.0\n",
            "          Matplotlib (3.2.2) : actual: 3.7.1\n",
            "                   CPU count : 12\n",
            "                   CPU speed : 2.20 GHz\n",
            "               CPU max speed : 0.00 GHz\n",
            "---------------------------- : ----------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## (Optional) Export to .py"
      ],
      "metadata": {
        "id": "LaURlsasaGuo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pluto_chapter_5 = 'Data-Augmentation-with-Python/pluto/pluto_chapter_5.py'\n",
        "!cp {pluto_file} {pluto_chapter_5}"
      ],
      "metadata": {
        "id": "Gfx9Ai5Opyqp"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ‚úã Set up Kaggle username and app Key\n",
        "\n",
        "- Install the following libraries, and import it on the Notebook.\n",
        "- Follow by initialize Kaggle username, key and fetch methods.\n",
        "- STOP: Update your Kaggle access username or key first."
      ],
      "metadata": {
        "id": "kQ6ap39x8HyF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# %%CARRY-OVER code\n",
        "\n",
        "# -------------------- : --------------------\n",
        "# READ ME\n",
        "# Chapter 2 begin:\n",
        "# Install the following libraries, and import it on the Notebook.\n",
        "# Follow by initialize Kaggle username, key and fetch methods.\n",
        "# STOP: Update your Kaggle access username or key first.\n",
        "# -------------------- : --------------------\n",
        "\n",
        "!pip install opendatasets --upgrade\n",
        "import opendatasets\n",
        "print(\"\\nrequired version 0.1.22 or higher: \", opendatasets.__version__)\n",
        "\n",
        "!pip install pyspellchecker\n",
        "import spellchecker\n",
        "print(\"\\nRequired version 0.7+\", spellchecker.__version__)\n",
        "\n",
        "# STOP: Update your Kaggle access username or key first.\n",
        "pluto.remember_kaggle_access_keys(\"YOUR_KAGGLE_USERNAME\", \"YOUR_KAGGLE_API_KEY\")\n",
        "pluto._write_kaggle_credit()\n",
        "import kaggle\n",
        "\n",
        "@add_method(PacktDataAug)\n",
        "def fetch_kaggle_comp_data(self,cname):\n",
        "  #self._write_kaggle_credit()  # need to run only once.\n",
        "  path = pathlib.Path(cname)\n",
        "  kaggle.api.competition_download_cli(str(path))\n",
        "  zipfile.ZipFile(f'{path}.zip').extractall(path)\n",
        "  return\n",
        "\n",
        "@add_method(PacktDataAug)\n",
        "def fetch_kaggle_dataset(self,url,dest=\"kaggle\"):\n",
        "  #self._write_kaggle_credit()    # need to run only once.\n",
        "  opendatasets.download(url,data_dir=dest)\n",
        "  return\n",
        "# -------------------- : --------------------\n"
      ],
      "metadata": {
        "id": "sf4Obdp-77CL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c08eec23-68be-4b2a-8b39-aeb4c24e1698"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: opendatasets in /usr/local/lib/python3.10/dist-packages (0.1.22)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from opendatasets) (4.66.1)\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.10/dist-packages (from opendatasets) (1.5.16)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from opendatasets) (8.1.7)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (1.16.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (2023.7.22)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (2.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (2.31.0)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (8.0.1)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (2.0.4)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (6.0.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->kaggle->opendatasets) (0.5.1)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.10/dist-packages (from python-slugify->kaggle->opendatasets) (1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle->opendatasets) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle->opendatasets) (3.4)\n",
            "\n",
            "required version 0.1.22 or higher:  0.1.22\n",
            "Requirement already satisfied: pyspellchecker in /usr/local/lib/python3.10/dist-packages (0.7.2)\n",
            "\n",
            "Required version 0.7+ 0.7.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fetch Kaggle Data"
      ],
      "metadata": {
        "id": "5BfHiMQE8XWp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## NetFlix"
      ],
      "metadata": {
        "id": "WSWTfhPU9YjQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "url = 'https://www.kaggle.com/datasets/infamouscoder/dataset-netflix-shows'\n",
        "pluto.fetch_kaggle_dataset(url)"
      ],
      "metadata": {
        "id": "_TEFhsYs77Go",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45bc3179-e06d-4d71-cdf1-6cb1b3ca3fdd"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading dataset-netflix-shows.zip to kaggle/dataset-netflix-shows\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.34M/1.34M [00:01<00:00, 1.38MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "CPU times: user 203 ms, sys: 12 ms, total: 215 ms\n",
            "Wall time: 2.1 s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "f = 'kaggle/dataset-netflix-shows/netflix_titles.csv'\n",
        "pluto.df_netflix_data = pluto.fetch_df(f)\n",
        "pluto.df_netflix_data.head(3)"
      ],
      "metadata": {
        "id": "_GHi9DnM77Kj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pluto.print_batch_text(pluto.df_netflix_data)"
      ],
      "metadata": {
        "id": "FJP18_-M77Nw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pluto.count_word(pluto.df_netflix_data)\n",
        "pluto.df_netflix_data.head()"
      ],
      "metadata": {
        "id": "EC_P1qsP77RG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pluto.draw_word_count(pluto.df_netflix_data)"
      ],
      "metadata": {
        "id": "HTXz32vF6_cn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %%CARRY-OVER code install\n",
        "\n",
        "!pip install missingno"
      ],
      "metadata": {
        "id": "QAxo1zXThdkM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b33b87b8-96f6-4eab-a0e1-0b54d2343462"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: missingno in /usr/local/lib/python3.10/dist-packages (0.5.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from missingno) (1.23.5)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from missingno) (3.7.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from missingno) (1.11.2)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.10/dist-packages (from missingno) (0.12.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->missingno) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->missingno) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->missingno) (4.42.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->missingno) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->missingno) (23.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->missingno) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->missingno) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->missingno) (2.8.2)\n",
            "Requirement already satisfied: pandas>=0.25 in /usr/local/lib/python3.10/dist-packages (from seaborn->missingno) (1.5.3)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.25->seaborn->missingno) (2023.3.post1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->missingno) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %%writefile -a {pluto_chapter_5}\n",
        "\n",
        "# prompt: write detail Python documentation with default value for the following function: draw_text_null_data\n",
        "pluto.version = 5.0\n",
        "import missingno\n",
        "@add_method(PacktDataAug)\n",
        "def draw_text_null_data(self, df, color=(0.3,0.36,0.44)):\n",
        "\n",
        "  \"\"\"\n",
        "  Draws a heatmap of missing values for a given Pandas DataFrame.\n",
        "\n",
        "  Args:\n",
        "    df (Pandas DataFrame): The data to draw a heatmap of from.\n",
        "    color (tuple, optional): The color for the heatmap. Defaults to (0.3,0.36,0.44).\n",
        "\n",
        "  Returns:\n",
        "    None.\n",
        "  \"\"\"\n",
        "  canvas, pic = matplotlib.pyplot.subplots(1, 1, figsize=(10, 6))\n",
        "  missingno.matrix(df,color=color,ax=pic)\n",
        "  pic.set_title('Missing Data (Null Value)')\n",
        "  pic.set_xlabel('Solid is has data. White line is missing/null data.')\n",
        "  canvas.tight_layout()\n",
        "  self._drop_image(canvas)\n",
        "  canvas.show()\n",
        "  return"
      ],
      "metadata": {
        "id": "uN3KPZBbhadh"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pluto.draw_text_null_data(pluto.df_netflix_data)"
      ],
      "metadata": {
        "id": "-MaTeOMghuR2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %%CARRY-OVER code install\n",
        "\n",
        "!pip install nltk\n",
        "!pip install wordcloud\n",
        "#\n",
        "# tested with the following version\n",
        "# !pip install nltk==3.8.1\n",
        "# !pip install wordcloud==1.8.2.2"
      ],
      "metadata": {
        "id": "oBGVKXYGh_z-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ac52dea-014b-4eb3-cd92-25ae3ddbf74b"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.1)\n",
            "Requirement already satisfied: wordcloud in /usr/local/lib/python3.10/dist-packages (1.9.2)\n",
            "Requirement already satisfied: numpy>=1.6.1 in /usr/local/lib/python3.10/dist-packages (from wordcloud) (1.23.5)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from wordcloud) (9.4.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from wordcloud) (3.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->wordcloud) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->wordcloud) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->wordcloud) (4.42.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->wordcloud) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->wordcloud) (23.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->wordcloud) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->wordcloud) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->wordcloud) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %%writefile -a {pluto_chapter_5}\n",
        "\n",
        "# prompt: write detail Python documentation with default value for the following function: _draw_text_wordcloud\n",
        "import nltk\n",
        "import wordcloud\n",
        "import re\n",
        "@add_method(PacktDataAug)\n",
        "def _draw_image_wordcloud(self, words_str, xignore_words='cat', title='Word Cloud:'):\n",
        "  \"\"\"\n",
        "  Draws a word cloud from a given string of text.\n",
        "\n",
        "  Args:\n",
        "    words_str (string): The data string format seperate by space.\n",
        "    xignore_words (str, optional): Any word(s) that should be ignored. Defaults to 'cat'.\n",
        "    title (str, optional): The title of the plot. Defaults to 'Word Cloud:'.\n",
        "\n",
        "  Returns:\n",
        "    None.\n",
        "  \"\"\"\n",
        "  canvas, pic = matplotlib.pyplot.subplots(1, 1, figsize=(16, 8))\n",
        "  img = wordcloud.WordCloud(width = 1400,\n",
        "    height = 800,\n",
        "    background_color ='white',\n",
        "    stopwords = xignore_words,\n",
        "    min_font_size = 10).generate(words_str)\n",
        "  pic.imshow(img)\n",
        "  pic.set_title(title)\n",
        "  pic.set_xlabel(f'Approximate Words: {int(len(words_str) / 5)}')\n",
        "  pic.tick_params(left = False, right = False, labelleft = False,\n",
        "    labelbottom = False, bottom = False)\n",
        "  canvas.tight_layout()\n",
        "  self._drop_image(canvas)\n",
        "  canvas.show()\n",
        "  return\n",
        "  #\n",
        "# prompt: write detail Python documentation with default value for the following function: draw_text_wordcloud\n",
        "@add_method(PacktDataAug)\n",
        "def draw_text_wordcloud(self, df_1column, xignore_words='cat', title='Word Cloud:'):\n",
        "  \"\"\"\n",
        "  Draws a word cloud from a given column of text. It uses the helper function:\n",
        "  _draw_image_wordcloud().\n",
        "\n",
        "  Args:\n",
        "    df_1column (Pandas DataFrame): The data to draw a heatmap of from.\n",
        "    xignore_words (str, optional): Any word(s) that should be ignored. Defaults to 'cat'.\n",
        "    title (str, optional): The title of the plot. Defaults to 'Word Cloud:'.\n",
        "\n",
        "  Returns:\n",
        "    None.\n",
        "  \"\"\"\n",
        "  orig = df_1column.str.cat()\n",
        "  clean = re.sub('[^A-Za-z0-9 ]+', '', orig)\n",
        "  self._draw_image_wordcloud(clean, xignore_words=xignore_words,title=title)\n",
        "  return"
      ],
      "metadata": {
        "id": "f2Y799hsv7Kf"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Nltk version 3.7: actual: ', nltk.__version__)\n",
        "print('WordCloud version 1.8.2.2: actual: ', wordcloud.__version__)"
      ],
      "metadata": {
        "id": "bSauNuC1wGzJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pluto.draw_text_wordcloud(pluto.df_netflix_data.description,\n",
        "  xignore_words=wordcloud.STOPWORDS,\n",
        "  title='Word Cloud: Netflix Movie Review')"
      ],
      "metadata": {
        "id": "OaGD2wtPANLR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Twitter"
      ],
      "metadata": {
        "id": "ZurnPA1u-T8z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @add_method(PacktDataAug)\n",
        "# def fetch_df(self, csv):\n",
        "#   df = pandas.read_csv(csv, encoding='latin-1')\n",
        "#   return df"
      ],
      "metadata": {
        "id": "FRjmqGVn-msp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "%%time\n",
        "url = 'https://www.kaggle.com/datasets/mayurdalvi/twitter-sentiments-analysis-nlp'\n",
        "pluto.fetch_kaggle_dataset(url)"
      ],
      "metadata": {
        "id": "WePFW0tu6_fe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# remove white space in directory and filename\n",
        "# run this until no error/output\n",
        "f = 'kaggle/twitter-sentiments-analysis-nlp'\n",
        "#!find {f} -name \"* *\" -type d | rename 's/ /_/g'\n",
        "!find {f} -name \"* *\" -type f | rename 's/ /_/g'"
      ],
      "metadata": {
        "id": "ms7jgEcrKpIr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f = 'kaggle/twitter-sentiments-analysis-nlp/Twitter_Sentiments.csv'\n",
        "pluto.df_twitter_data = pluto.fetch_df(f)\n",
        "pluto.df_twitter_data.head(3)"
      ],
      "metadata": {
        "id": "HhaenARf-mmJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pluto.print_batch_text(pluto.df_twitter_data,cols=['label', 'tweet'])"
      ],
      "metadata": {
        "id": "onbBfDI--mvl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %%CARRY-OVER code install\n",
        "\n",
        "!pip install filter-profanity\n",
        "#\n",
        "# tested on the following on version:\n",
        "#!pip install filter-profanity==1.0.9"
      ],
      "metadata": {
        "id": "E7s_6RTlBXJr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d520c13f-a29e-4592-8264-0a80a81e3324"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting filter-profanity\n",
            "  Downloading filter-profanity-1.0.9.tar.gz (4.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: filter-profanity\n",
            "  Building wheel for filter-profanity (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for filter-profanity: filename=filter_profanity-1.0.9-py3-none-any.whl size=5199 sha256=d0104de21ef1c6d53ab49250f80047a017749d97c207478a3e22c4ae1f2608e9\n",
            "  Stored in directory: /root/.cache/pip/wheels/62/c4/67/6c6cabc8e2202296e219178d7b6304d893a8b884f5b70406b6\n",
            "Successfully built filter-profanity\n",
            "Installing collected packages: filter-profanity\n",
            "Successfully installed filter-profanity-1.0.9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %%writefile -a {pluto_chapter_5}\n",
        "\n",
        "# prompt: write detail Python documentation with default value for the following function: _clean_text\n",
        "import profanity\n",
        "import re\n",
        "#\n",
        "@add_method(PacktDataAug)\n",
        "def _clean_text(self,x):\n",
        "  \"\"\"\n",
        "  Cleans a text by removing any special characters.\n",
        "\n",
        "  Args:\n",
        "    x (str): The text to be cleaned.\n",
        "\n",
        "  Returns:\n",
        "    str: The cleaned text.\n",
        "  \"\"\"\n",
        "  return (re.sub('[^A-Za-z0-9 .,!?#@]+', '', str(x)))\n",
        "#\n",
        "# prompt: write detail Python documentation with default value for the following function: _clean_bad_word\n",
        "@add_method(PacktDataAug)\n",
        "def _clean_bad_word(self,x):\n",
        "\n",
        "  \"\"\"\n",
        "  Cleans a text by replacing any bad word with a '***' character. It uses the\n",
        "  cesnsor_profanity() function from profanity library .\n",
        "\n",
        "  Args:\n",
        "    x (str): The text to be cleaned.\n",
        "\n",
        "  Returns:\n",
        "    str: The cleaned text.\n",
        "  \"\"\"\n",
        "\n",
        "  return (profanity.censor_profanity(x, ''))\n",
        "#\n",
        "# prompt: write detail Python documentation with default value for the following function: clean_text\n",
        "@add_method(PacktDataAug)\n",
        "def clean_text(self, df):\n",
        "\n",
        "  \"\"\"\n",
        "  Cleans a text by replacing any special characters with a '_' character and\n",
        "  replaces any bad word with a '***' character.\n",
        "  It uses the _clean_text() and _clean_bad_word() helper functions.\n",
        "\n",
        "  Args:\n",
        "    df (Pandas DataFrame): The Pandas DataFrame containing the column 'tweet'\n",
        "    which is to be cleaned.\n",
        "\n",
        "  Returns:\n",
        "    Pandas DataFrame: The modified Pandas DataFrame with a new column 'clean_tweet'.\n",
        "  \"\"\"\n",
        "  df['clean_tweet'] = df.tweet.apply(self._clean_text)\n",
        "  df['clean_tweet'] = df['clean_tweet'].apply(self._clean_bad_word)\n",
        "  return df"
      ],
      "metadata": {
        "id": "R7NuXRci5wC9"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "pluto.clean_text(pluto.df_twitter_data)\n",
        "pluto.df_twitter_data.head()\n",
        "#\n",
        "pluto.df_netflix_data['description'] = pluto.df_netflix_data['description'].apply(pluto._clean_text)"
      ],
      "metadata": {
        "id": "RDqqtRY883yk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pluto.print_batch_text(pluto.df_twitter_data, cols=['label', 'clean_tweet'])"
      ],
      "metadata": {
        "id": "ViaQTmnBc-0U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# double check on clean tweets\n",
        "print('clean: ', pluto.df_twitter_data.clean_tweet[13538], ' : original: ',\n",
        "  pluto.df_twitter_data.tweet[13538], ': label: ', pluto.df_twitter_data.label[13538])"
      ],
      "metadata": {
        "id": "3cVkaqIveWwU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# double check on clean tweets\n",
        "with pandas.option_context(\"display.max_colwidth\", None):\n",
        "  display(pluto.df_twitter_data[pluto.df_twitter_data.label == 1].sample(10))"
      ],
      "metadata": {
        "id": "8uKcbMAefmfN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pluto.count_word(pluto.df_twitter_data,col_dest='clean_tweet')\n",
        "pluto.draw_word_count(pluto.df_twitter_data)"
      ],
      "metadata": {
        "id": "wsyzuXJ_-my0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pluto.draw_text_null_data(pluto.df_twitter_data)"
      ],
      "metadata": {
        "id": "pzqZtxaIh2lw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pluto.draw_text_wordcloud(pluto.df_twitter_data.clean_tweet,\n",
        "  xignore_words=wordcloud.STOPWORDS,\n",
        "  title='Clean Tweets Word Cloud')"
      ],
      "metadata": {
        "id": "QBGppWuskKZf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Export (aka drop or save) data file"
      ],
      "metadata": {
        "id": "iubboKaOM5-Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# %%writefile -a {pluto_chapter_5}\n",
        "\n",
        "# prompt: write detail Python documentation with default value for the following function: _drop_df_file\n",
        "@add_method(PacktDataAug)\n",
        "def _drop_df_file(self, df,fname,type='csv',sep='~'):\n",
        "  \"\"\"\n",
        "  Convert a Pandas dataframe to a file. It uses the Pandas dataframe .to_csv()\n",
        "  function. It adds an option to specify the file type. The default file type\n",
        "  is 'csv'.\n",
        "\n",
        "  Args:\n",
        "    df (Pandas DataFrame): The Pandas DataFrame to be dropped to a file.\n",
        "    fname (str): The name of the file to which the Pandas DataFrame\n",
        "      is to be created.\n",
        "    type (str, optional): The file type. The default file type is 'csv'.\n",
        "    sep (str, optional): The separator character for CSV file. The default\n",
        "      separator is '~'.\n",
        "\n",
        "  Returns:\n",
        "    None.\n",
        "  \"\"\"\n",
        "  df.to_csv(fname,sep=sep)\n",
        "  return"
      ],
      "metadata": {
        "id": "oVO43N-Dh2pL"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f = 'Data-Augmentation-with-Python/pluto_data/netflix_data.csv'\n",
        "pluto._drop_df_file(pluto.df_netflix_data, f)"
      ],
      "metadata": {
        "id": "FDswusz3LTfK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f = 'Data-Augmentation-with-Python/pluto_data/twitter_data.csv'\n",
        "pluto._drop_df_file(pluto.df_twitter_data, f)"
      ],
      "metadata": {
        "id": "EmYEusRiMkSD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oZqCjjxuOvZN"
      },
      "source": [
        "# Character Augmenter<a class=\"anchor\" id=\"chara_aug\">\n",
        "\n",
        "Augmenting data in character level. Possible scenarios include image to text and chatbot. During recognizing text from image, we need to optical character recognition (OCR) model to achieve it but OCR introduces some errors such as recognizing \"o\" and \"0\". `OCRAug` simulate these errors to perform the data augmentation. For chatbot, we still have typo even though most of application comes with word correction. Therefore, `KeyboardAug` is introduced to simulate this kind of errors."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %%CARRY-OVER code install\n",
        "\n",
        "!pip install nlpaug\n",
        "#\n",
        "# tested on the following version:\n",
        "#!pip install nlpaug==1.1.11"
      ],
      "metadata": {
        "id": "rtHdmFVaPBtC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17a92626-b290-4e00-c2b9-b0852ecceada"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nlpaug\n",
            "  Downloading nlpaug-1.1.11-py3-none-any.whl (410 kB)\n",
            "\u001b[?25l     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/410.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[91m‚ï∏\u001b[0m\u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m92.2/410.5 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m410.5/410.5 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.16.2 in /usr/local/lib/python3.10/dist-packages (from nlpaug) (1.23.5)\n",
            "Requirement already satisfied: pandas>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from nlpaug) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.22.0 in /usr/local/lib/python3.10/dist-packages (from nlpaug) (2.31.0)\n",
            "Requirement already satisfied: gdown>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from nlpaug) (4.6.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown>=4.0.0->nlpaug) (3.12.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from gdown>=4.0.0->nlpaug) (1.16.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gdown>=4.0.0->nlpaug) (4.66.1)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown>=4.0.0->nlpaug) (4.11.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2.0->nlpaug) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2.0->nlpaug) (2023.3.post1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->nlpaug) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->nlpaug) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->nlpaug) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->nlpaug) (2023.7.22)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown>=4.0.0->nlpaug) (2.5)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->nlpaug) (1.7.1)\n",
            "Installing collected packages: nlpaug\n",
            "Successfully installed nlpaug-1.1.11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %%writefile -a {pluto_chapter_5}\n",
        "\n",
        "import nlpaug\n",
        "import nlpaug.augmenter\n",
        "import nlpaug.augmenter.char\n",
        "import nlpaug.augmenter.word"
      ],
      "metadata": {
        "id": "4gs85bUcGTXx"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('version 1.1.11, actual: ',nlpaug.__version__)"
      ],
      "metadata": {
        "id": "TS8IO9pWo40i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %%writefile -a {pluto_chapter_5}\n",
        "\n",
        "pluto.orig_text = 'It was the best of times. It was the worst of times. It was the age of wisdom. It was the age of foolishness. It was the epoch of belief. It was the epoch of incredulity.'"
      ],
      "metadata": {
        "id": "6nl9A5n1G_WS"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %%writefile -a {pluto_chapter_5}\n",
        "\n",
        "# prompt: write detail Python documentation with default value for the following function: _print_aug_batch\n",
        "@add_method(PacktDataAug)\n",
        "def _print_aug_batch(self, df, aug_func, col_dest=\"description\",\n",
        "  bsize=3, aug_name='Augmented',is_larger_font=True):\n",
        "\n",
        "  \"\"\"\n",
        "  Prints a batch of data augmentation results.\n",
        "\n",
        "  Args:\n",
        "    df (Pandas DataFrame): The Pandas DataFrame containing the column to be\n",
        "      augmented.\n",
        "    aug_func (nlpaug.augmenter.Augmenter): The nlpaug augmenter to be used for\n",
        "      data augmentation.\n",
        "    col_dest (str, optional): The column name of the column to be augmented.\n",
        "      The default column name is 'description'.\n",
        "    bsize (int, optional): The batch size of the data augmentation results to be\n",
        "      printed. The default batch size is 3.\n",
        "    aug_name (str, optional): The name of the data augmentation function. The\n",
        "      default name is 'Augmented'.\n",
        "    is_larger_font (bool, optional): Whether to use a larger font for printing\n",
        "      the data augmentation results. The default is True.\n",
        "\n",
        "  Returns:\n",
        "    None.\n",
        "  \"\"\"\n",
        "\n",
        "  col_name = [aug_name, 'Original']\n",
        "  aug = aug_func.augment(self.orig_text, n=1)\n",
        "  data = [[aug[0], self.orig_text]]\n",
        "  df_aug = pandas.DataFrame(data, columns=col_name)\n",
        "  orig = df[col_dest].sample(bsize)\n",
        "  for tx in orig:\n",
        "    aug = aug_func.augment(tx, n=1)\n",
        "    data = [[aug[0], tx]]\n",
        "    t = pandas.DataFrame(data, columns=col_name)\n",
        "    df_aug = df_aug.append(t, ignore_index=True)\n",
        "  #\n",
        "  with pandas.option_context(\"display.max_colwidth\", None):\n",
        "    if (is_larger_font):\n",
        "      display(df_aug.head(bsize+1).style.set_table_styles(self._fetch_larger_font()))\n",
        "    else:\n",
        "      display(df_aug.head(bsize+1))\n",
        "  return"
      ],
      "metadata": {
        "id": "_OhmyNQRHIfS"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## OCR augmenting"
      ],
      "metadata": {
        "id": "sAX4XyY6bpW3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# %%writefile -a {pluto_chapter_5}\n",
        "\n",
        "# prompt: write detail Python documentation with default value for the following function: print_aug_ocr\n",
        "@add_method(PacktDataAug)\n",
        "def print_aug_ocr(self, df, col_dest=\"description\",bsize=3, aug_name='Augmented'):\n",
        "\n",
        "  \"\"\"\n",
        "  Performs OCR augmentation on the given text data and prints the results.\n",
        "\n",
        "  Args:\n",
        "    df (Pandas DataFrame): The Pandas DataFrame containing the column to be\n",
        "      augmented.\n",
        "    col_dest (str, optional): The column name of the column to be augmented.\n",
        "      The default column name is 'description'.\n",
        "    bsize (int, optional): The batch size of the data augmentation results to be\n",
        "      printed. The default batch size is 3.\n",
        "    aug_name (str, optional): The name of the data augmentation function. The\n",
        "      default name is 'Augmented'.\n",
        "\n",
        "  Returns:\n",
        "    None.\n",
        "  \"\"\"\n",
        "  aug_func = nlpaug.augmenter.char.OcrAug()\n",
        "  self._print_aug_batch(df, aug_func,col_dest=col_dest,bsize=bsize, aug_name=aug_name)\n",
        "  return"
      ],
      "metadata": {
        "id": "RUz21pdqYv3i"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pluto.print_aug_ocr(pluto.df_netflix_data, col_dest='description',aug_name='OCR Augment')"
      ],
      "metadata": {
        "id": "sb_tMYrxY9nV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pluto.print_aug_ocr(pluto.df_twitter_data, col_dest='clean_tweet',aug_name='OCR Augment')"
      ],
      "metadata": {
        "id": "NtOYQz1FY9qy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uHDd-yZcOvZQ"
      },
      "source": [
        "## Keyboard Augmenter<a class=\"anchor\" id=\"keyboard_aug\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bOg0vDdbOvZQ"
      },
      "source": [
        "- Substitute character by keyboard distance"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %%writefile -a {pluto_chapter_5}\n",
        "\n",
        "# prompt: write detail Python documentation with default value for the following function: print_aug_keyboard\n",
        "@add_method(PacktDataAug)\n",
        "def print_aug_keyboard(self, df, col_dest=\"description\",bsize=3, aug_name='Keyboard Augment'):\n",
        "\n",
        "  \"\"\"\n",
        "  Performs keyboard augmentation on the given text data and prints the results.\n",
        "\n",
        "  Args:\n",
        "    df (Pandas DataFrame): The Pandas DataFrame containing the column to be\n",
        "      augmented.\n",
        "    col_dest (str, optional): The column name of the column to be augmented.\n",
        "      The default column name is 'description'.\n",
        "    bsize (int, optional): The batch size of the data augmentation results to be\n",
        "      printed. The default batch size is 3.\n",
        "    aug_name (str, optional): The name of the data augmentation function. The\n",
        "      default name is 'Keyboard Augment'.\n",
        "\n",
        "  Returns:\n",
        "    None.\n",
        "  \"\"\"\n",
        "  aug_func = nlpaug.augmenter.char.KeyboardAug()\n",
        "  self._print_aug_batch(df, aug_func,col_dest=col_dest,bsize=bsize, aug_name=aug_name)\n",
        "  return"
      ],
      "metadata": {
        "id": "YlB5M1oPeDpe"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pluto.print_aug_keyboard(pluto.df_netflix_data, col_dest='description',\n",
        "  aug_name='Keyboard Augment')"
      ],
      "metadata": {
        "id": "hWOa5vpweVKl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pluto.print_aug_keyboard(pluto.df_twitter_data, col_dest='clean_tweet',\n",
        "  aug_name='Keyboard Augment')"
      ],
      "metadata": {
        "id": "0_nCXoCAeVN0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07pLIh0ZOvZR"
      },
      "source": [
        "## Random Augmenter<a class=\"anchor\" id=\"random_aug\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %%writefile -a {pluto_chapter_5}\n",
        "\n",
        "# prompt: write detail Python documentation with default value for the following function: print_aug_char_random\n",
        "@add_method(PacktDataAug)\n",
        "def print_aug_char_random(self, df, action='insert', col_dest=\"description\",bsize=3, aug_name='Augment'):\n",
        "\n",
        "  \"\"\"\n",
        "  Performs random augmentation on the given text data and prints the results.\n",
        "\n",
        "  Args:\n",
        "    df (Pandas DataFrame): The Pandas DataFrame containing the column to be\n",
        "      augmented.\n",
        "    col_dest (str, optional): The column name of the column to be augmented.\n",
        "      The default column name is 'description'.\n",
        "    bsize (int, optional): The batch size of the data augmentation results to be\n",
        "      printed. The default batch size is 3.\n",
        "    aug_name (str, optional): The name of the data augmentation function. The\n",
        "      default name is 'Augmented'.\n",
        "    action (str, optional): The action of augmentation. Possible values include:\n",
        "      - 'insert': Insert random characters in the target text.\n",
        "      - 'delete': Delete random characters in the target text.\n",
        "      - 'substitute': Substitute random characters in the target text.\n",
        "      The default value is 'insert'.\n",
        "\n",
        "  Returns:\n",
        "    None.\n",
        "  \"\"\"\n",
        "  aug_func = nlpaug.augmenter.char.RandomCharAug(action=action)\n",
        "  self._print_aug_batch(df, aug_func,col_dest=col_dest,bsize=bsize, aug_name=aug_name)\n",
        "  return"
      ],
      "metadata": {
        "id": "3yEFMY17guY_"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pluto.print_aug_char_random(pluto.df_netflix_data, action='insert',\n",
        "  col_dest='description', aug_name='Random Insert Augment')"
      ],
      "metadata": {
        "id": "yeGr1b7qTaoB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pluto.print_aug_char_random(pluto.df_netflix_data, action='delete',\n",
        "  col_dest='description', aug_name='Random Delete Augment')"
      ],
      "metadata": {
        "id": "IiXwaqp2Ta0e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pluto.print_aug_char_random(pluto.df_netflix_data, action='substitute',\n",
        "  col_dest='description', aug_name='Random Substitute Augment')"
      ],
      "metadata": {
        "id": "J9oLzmMqUDTt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pluto.print_aug_char_random(pluto.df_netflix_data, action='swap',\n",
        "  col_dest='description', aug_name='Random Swap Augment')"
      ],
      "metadata": {
        "id": "wMnc0_4tUDXL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pluto.print_aug_char_random(pluto.df_twitter_data, action='insert',\n",
        "  col_dest='clean_tweet', aug_name='Random Insert Augment')"
      ],
      "metadata": {
        "id": "vmemvezFUDaP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pluto.print_aug_char_random(pluto.df_twitter_data, action='delete',\n",
        "  col_dest='clean_tweet', aug_name='Random Delete Augment')"
      ],
      "metadata": {
        "id": "AtuG0YkdUDdp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pluto.print_aug_char_random(pluto.df_twitter_data, action='substitute',\n",
        "  col_dest='clean_tweet', aug_name='Random Substitute Augment')"
      ],
      "metadata": {
        "id": "nLM2luSNUDgW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pluto.print_aug_char_random(pluto.df_twitter_data, action='swap',\n",
        "  col_dest='clean_tweet', aug_name='Random Swap Augment')"
      ],
      "metadata": {
        "id": "dXQmgum3UDjH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Word augmentation"
      ],
      "metadata": {
        "id": "YH7uYnPvzaWn"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YfCX_OBFOvZV"
      },
      "source": [
        "## Misspell Augmenter<a class=\"anchor\" id=\"spelling_aug\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "98-Qs4wUOvZV"
      },
      "source": [
        "- Substitute word by spelling mistake words dictionary"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %%writefile -a {pluto_chapter_5}\n",
        "\n",
        "# prompt: write detail Python documentation with default value for the following function: print_aug_word_misspell\n",
        "@add_method(PacktDataAug)\n",
        "def print_aug_word_misspell(self, df, col_dest=\"description\",bsize=3, aug_name='Augment'):\n",
        "\n",
        "  \"\"\"\n",
        "  Performs misspell augmentation on the given text data and prints the results.\n",
        "\n",
        "  Args:\n",
        "    df (Pandas DataFrame): The Pandas DataFrame containing the column to be\n",
        "      augmented.\n",
        "    col_dest (str, optional): The column name of the column to be augmented.\n",
        "      The default column name is 'description'.\n",
        "    bsize (int, optional): The batch size of the data augmentation results to be\n",
        "      printed. The default batch size is 3.\n",
        "    aug_name (str, optional): The name of the data augmentation function. The\n",
        "      default name is 'Augmented'.\n",
        "\n",
        "  Returns:\n",
        "    None.\n",
        "  \"\"\"\n",
        "  aug_func = nlpaug.augmenter.word.SpellingAug()\n",
        "  self._print_aug_batch(df, aug_func,col_dest=col_dest,bsize=bsize, aug_name=aug_name)\n",
        "  return"
      ],
      "metadata": {
        "id": "cKXwiAXgWO8z"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pluto.print_aug_word_misspell(pluto.df_netflix_data,\n",
        "  col_dest='description', aug_name='Word Spelling Augment')"
      ],
      "metadata": {
        "id": "ha0e6lcdXVPs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pluto.print_aug_word_misspell(pluto.df_twitter_data,\n",
        "  col_dest='clean_tweet', aug_name='Word Spelling Augment')"
      ],
      "metadata": {
        "id": "ohHaLHRHXvtL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AcPTut6FOvZi"
      },
      "source": [
        "## Split Augmenter<a class=\"anchor\" id=\"split_aug\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iZJhHv9HOvZi"
      },
      "source": [
        "- Split word to two tokens randomly"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %%writefile -a {pluto_chapter_5}\n",
        "\n",
        "# prompt: write detail Python documentation with default value for the following function: print_aug_word_split\n",
        "@add_method(PacktDataAug)\n",
        "def print_aug_word_split(self, df, col_dest=\"description\",bsize=3, aug_name='Augment'):\n",
        "\n",
        "  \"\"\"\n",
        "  Performs split augmentation on the given text data and prints the results.\n",
        "\n",
        "  Args:\n",
        "    df (Pandas DataFrame): The Pandas DataFrame containing the column to be\n",
        "      augmented.\n",
        "    col_dest (str, optional): The column name of the column to be augmented.\n",
        "      The default column name is 'description'.\n",
        "    bsize (int, optional): The batch size of the data augmentation results to be\n",
        "      printed. The default batch size is 3.\n",
        "    aug_name (str, optional): The name of the data augmentation function. The\n",
        "      default name is 'Augmented'.\n",
        "\n",
        "  Returns:\n",
        "    None.\n",
        "  \"\"\"\n",
        "  aug_func = nlpaug.augmenter.word.SplitAug()\n",
        "  self._print_aug_batch(df, aug_func,col_dest=col_dest,bsize=bsize, aug_name=aug_name)\n",
        "  return"
      ],
      "metadata": {
        "id": "2D0ybu_1YDog"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pluto.print_aug_word_split(pluto.df_netflix_data, col_dest='description', aug_name='Word Split Augment')"
      ],
      "metadata": {
        "id": "IML8dabSYRc7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pluto.print_aug_word_split(pluto.df_twitter_data, col_dest='clean_tweet', aug_name='Word Split Augment')"
      ],
      "metadata": {
        "id": "lTfSH_rPYRgW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GOjTDye8OvZf"
      },
      "source": [
        "## Random Word Augmenter"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %%writefile -a {pluto_chapter_5}\n",
        "\n",
        "# prompt: write detail Python documentation with default value for the following function: print_aug_word_random\n",
        "@add_method(PacktDataAug)\n",
        "def print_aug_word_random(self, df, action='swap', col_dest=\"description\",bsize=3, aug_name='Augment'):\n",
        "\n",
        "  \"\"\"\n",
        "  Performs random word augmentation on the given text data and prints the results.\n",
        "\n",
        "  Args:\n",
        "    df (Pandas DataFrame): The Pandas DataFrame containing the column to be\n",
        "      augmented.\n",
        "    col_dest (str, optional): The column name of the column to be augmented.\n",
        "      The default column name is 'description'.\n",
        "    bsize (int, optional): The batch size of the data augmentation results to be\n",
        "      printed. The default batch size is 3.\n",
        "    aug_name (str, optional): The name of the data augmentation function. The\n",
        "      default name is 'Augmented'.\n",
        "    action (str, optional): The action of augmentation. Possible values include:\n",
        "      - 'swap': Swap two random words in the target text.\n",
        "      - 'substitute': Substitute a random word with another random word from the\n",
        "        vocab.\n",
        "      - 'insert': Insert a random word at a random position in the target text.\n",
        "      - 'delete': Delete a random word from the target text.\n",
        "      The default value is 'swap'.\n",
        "\n",
        "  Returns:\n",
        "    None.\n",
        "  \"\"\"\n",
        "  aug_func = nlpaug.augmenter.word.RandomWordAug(action=action)\n",
        "  self._print_aug_batch(df, aug_func,col_dest=col_dest,bsize=bsize, aug_name=aug_name)\n",
        "  return"
      ],
      "metadata": {
        "id": "OP0EgFV1ZS4n"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pluto.print_aug_word_random(pluto.df_netflix_data, action='swap',\n",
        "  col_dest='description', aug_name='Word Random Swap Augment')"
      ],
      "metadata": {
        "id": "t0_1pjpuaX9C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pluto.print_aug_word_random(pluto.df_twitter_data, action='swap',\n",
        "  col_dest='clean_tweet', aug_name='Word Random Swap Augment')"
      ],
      "metadata": {
        "id": "6kFTA5XkaYK6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pluto.print_aug_word_random(pluto.df_netflix_data, action='substitute',\n",
        "  col_dest='description', aug_name='Word Random Substitude Augment')"
      ],
      "metadata": {
        "id": "ux703ByVaYA7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pluto.print_aug_word_random(pluto.df_twitter_data, action='substitute',\n",
        "  col_dest='clean_tweet', aug_name='Word Random Substitute Augment')"
      ],
      "metadata": {
        "id": "MLWj8WKzaYOz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pluto.print_aug_word_random(pluto.df_netflix_data, action='crop',\n",
        "  col_dest='description', aug_name='Word Random Crop Augment')"
      ],
      "metadata": {
        "id": "Fz7grRQYaYFf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pluto.print_aug_word_random(pluto.df_twitter_data, action='crop',\n",
        "  col_dest='clean_tweet', aug_name='Word Random Crop Augment')"
      ],
      "metadata": {
        "id": "g493xRrDaYSP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pluto.print_aug_word_random(pluto.df_netflix_data, action='delete',\n",
        "  col_dest='description', aug_name='Word Random Delete Augment')"
      ],
      "metadata": {
        "id": "jLfWfnT5aYH0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pluto.print_aug_word_random(pluto.df_twitter_data, action='delete',\n",
        "  col_dest='clean_tweet', aug_name='Word Random Delete Augment')"
      ],
      "metadata": {
        "id": "nETKRYFTaYVm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HjmXSJndOvZd"
      },
      "source": [
        "## Synonym Augmenter"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %%writefile -a {pluto_chapter_5}\n",
        "\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "83ODKdEYPowa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c12a6166-a10d-4dcc-9785-17c9b1734927"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yOImDH4QOvZd"
      },
      "source": [
        "- Substitute word by WordNet's synonym"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %%writefile -a {pluto_chapter_5}\n",
        "\n",
        "# prompt: write detail Python documentation with default value for the following function: print_aug_word_synonym\n",
        "@add_method(PacktDataAug)\n",
        "def print_aug_word_synonym(self, df, col_dest=\"description\",bsize=3, aug_name='Augment'):\n",
        "\n",
        "  \"\"\"\n",
        "  Performs synonym augmentation on the given text data and prints the results.\n",
        "\n",
        "  Args:\n",
        "    df (Pandas DataFrame): The Pandas DataFrame containing the column to be\n",
        "      augmented.\n",
        "    col_dest (str, optional): The column name of the column to be augmented.\n",
        "      The default column name is 'description'.\n",
        "    bsize (int, optional): The batch size of the data augmentation results to be\n",
        "      printed. The default batch size is 3.\n",
        "    aug_name (str, optional): The name of the data augmentation function. The\n",
        "      default name is 'Augmented'.\n",
        "\n",
        "  Returns:\n",
        "    None.\n",
        "  \"\"\"\n",
        "  aug_func = nlpaug.augmenter.word.SynonymAug(aug_src='wordnet')\n",
        "  self._print_aug_batch(df, aug_func,col_dest=col_dest,bsize=bsize, aug_name=aug_name)\n",
        "  return"
      ],
      "metadata": {
        "id": "QrJnm8k1dXAR"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pluto.print_aug_word_synonym(pluto.df_netflix_data,\n",
        "  col_dest='description', aug_name='Synonym WordNet Augment')"
      ],
      "metadata": {
        "id": "y075AG5jdand"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pluto.print_aug_word_synonym(pluto.df_twitter_data,\n",
        "  col_dest='clean_tweet', aug_name='Synonym WordNet Augment')"
      ],
      "metadata": {
        "id": "zOcCjlNkdaq1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XY7cu6wUOvZe"
      },
      "source": [
        "## Antonym Augmenter<a class=\"anchor\" id=\"antonym_aug\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %%writefile -a {pluto_chapter_5}\n",
        "\n",
        "# prompt: write detail Python documentation with default value for the following function: print_aug_word_antonym\n",
        "@add_method(PacktDataAug)\n",
        "def print_aug_word_antonym(self, df, col_dest=\"description\",bsize=3, aug_name='Augment'):\n",
        "\n",
        "  \"\"\"\n",
        "  Performs antonym augmentation on the given text data and prints the results.\n",
        "\n",
        "  Args:\n",
        "    df (Pandas DataFrame): The Pandas DataFrame containing the column to be\n",
        "      augmented.\n",
        "    col_dest (str, optional): The column name of the column to be augmented.\n",
        "      The default column name is 'description'.\n",
        "    bsize (int, optional): The batch size of the data augmentation results to be\n",
        "      printed. The default batch size is 3.\n",
        "    aug_name (str, optional): The name of the data augmentation function. The\n",
        "      default name is 'Augmented'.\n",
        "\n",
        "  Returns:\n",
        "    None.\n",
        "  \"\"\"\n",
        "  aug_func = nlpaug.augmenter.word.AntonymAug()\n",
        "  self._print_aug_batch(df, aug_func,col_dest=col_dest,bsize=bsize, aug_name=aug_name)\n",
        "  return"
      ],
      "metadata": {
        "id": "PvF8q32eeysk"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pluto.print_aug_word_antonym(pluto.df_netflix_data,\n",
        "  col_dest='description',aug_name='Antonym Augment')"
      ],
      "metadata": {
        "id": "7tqxTK-1eyvw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pluto.print_aug_word_antonym(pluto.df_twitter_data,\n",
        "  col_dest='clean_tweet',aug_name='Antonym Augment')"
      ],
      "metadata": {
        "id": "FGJEo3vHeyzB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kC0Vb6x9OvZk"
      },
      "source": [
        "## Reserved Word Augmenter<a class=\"anchor\" id=\"reserved_aug\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %%writefile -a {pluto_chapter_5}\n",
        "\n",
        "# prompt: write detail Python documentation with default value for the following function: print_aug_word_reserved\n",
        "@add_method(PacktDataAug)\n",
        "def print_aug_word_reserved(self, df, col_dest=\"description\",reserved_tokens=None,bsize=3, aug_name='Augment'):\n",
        "\n",
        "  \"\"\"\n",
        "  Performs reserved word augmentation on the given text data and prints the results.\n",
        "\n",
        "  Args:\n",
        "    df (Pandas DataFrame): The Pandas DataFrame containing the column to be\n",
        "      augmented.\n",
        "    col_dest (str, optional): The column name of the column to be augmented.\n",
        "      The default column name is 'description'.\n",
        "    reserved_tokens (list, optional): The list of words that will be reserved\n",
        "      and not be augmented. The default value is None.\n",
        "    bsize (int, optional): The batch size of the data augmentation results to be\n",
        "      printed. The default batch size is 3.\n",
        "    aug_name (str, optional): The name of the data augmentation function. The\n",
        "      default name is 'Augmented'.\n",
        "\n",
        "  Returns:\n",
        "    None.\n",
        "  \"\"\"\n",
        "  aug_func = nlpaug.augmenter.word.ReservedAug(reserved_tokens=reserved_tokens)\n",
        "  self._print_aug_batch(df, aug_func,col_dest=col_dest,bsize=bsize, aug_name=aug_name)\n",
        "  return"
      ],
      "metadata": {
        "id": "Vl927YB886BM"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %%writefile -a {pluto_chapter_5}\n",
        "\n",
        "pluto.reserved_control = [['wisdom', 'sagacity', 'intelligence', 'prudence'],\n",
        "  ['foolishness', 'folly', 'idiocy', 'stupidity']]"
      ],
      "metadata": {
        "id": "LasXSvw186OP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %%writefile -a {pluto_chapter_5}\n",
        "\n",
        "pluto.reserved_netflix = [['family','household', 'brood', 'unit', 'families'],\n",
        "  ['life','existance', 'entity', 'creation'],\n",
        "  ['love', 'warmth', 'endearment','tenderness']]\n",
        "pluto.reserved_netflix = pluto.reserved_control + pluto.reserved_netflix"
      ],
      "metadata": {
        "id": "Ilef_J_987PV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pluto.print_aug_word_reserved(pluto.df_netflix_data, col_dest='description',\n",
        "  reserved_tokens=pluto.reserved_netflix, aug_name='Netflix Reserved word augment')"
      ],
      "metadata": {
        "id": "bY2PVusC_ad7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %%writefile -a {pluto_chapter_5}\n",
        "\n",
        "pluto.reserved_twitter = [['user', 'users', 'customer', 'client','people','member','shopper'],\n",
        "  ['happy', 'cheerful', 'joyful', 'carefree'],\n",
        "  ['time','clock','hour']]\n",
        "pluto.reserved_twitter = pluto.reserved_control + pluto.reserved_twitter"
      ],
      "metadata": {
        "id": "y6ugzuz2_ahT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pluto.reserved_twitter"
      ],
      "metadata": {
        "id": "9n-tJ8qqD1rt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pluto.print_aug_word_reserved(pluto.df_twitter_data, col_dest='clean_tweet',\n",
        "  reserved_tokens=pluto.reserved_twitter,aug_name='Twitter Reserved word augment')"
      ],
      "metadata": {
        "id": "oJz48SZ9D1vC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# end of chapter 5\n",
        "print('End of chapter 5.')"
      ],
      "metadata": {
        "id": "ByK2v2hxOPLM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4520f82c-9649-4e3c-bb61-e65041d4f149"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "End of chapter 5.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# check on AI auto documentation\n",
        "help(pluto)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1LlqAljUcLus",
        "outputId": "e873011a-f3ee-4e57-8e94-95f9accac86b"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Help on PacktDataAug in module __main__ object:\n",
            "\n",
            "class PacktDataAug(builtins.object)\n",
            " |  PacktDataAug(name='Pluto', is_verbose=True, *args, **kwargs)\n",
            " |  \n",
            " |  The PacktDataAug class is the based class for the\n",
            " |  \"Data Augmentation with Python\" book.\n",
            " |  \n",
            " |  Methods defined here:\n",
            " |  \n",
            " |  __init__(self, name='Pluto', is_verbose=True, *args, **kwargs)\n",
            " |      This is the constructor function.\n",
            " |      \n",
            " |      Args:\n",
            " |      \n",
            " |       name (str): It requires a name for the object. The default is 'Pluto'\n",
            " |       verbose (bool):  The default value of `verbose` is True. This function prints out the\n",
            " |          name of the object if `is_verbose == True`. This is used to debug\n",
            " |          code. When you are ready to deploy the model, then you should set\n",
            " |          `is_verbose == False` in order to avoid printing out diagnostic\n",
            " |          messages.\n",
            " |      \n",
            " |        Additionally, this function takes any number of other\n",
            " |        parameters. These parameters are stored in `**kwargs` and are\n",
            " |        accessed via the function `get_kwargs()`. See the documentation\n",
            " |        for `get_kwargs()` for more details.\n",
            " |        Note that `__init__()` is\n",
            " |        automatically called when you create a new object.\n",
            " |      \n",
            " |      Returns:\n",
            " |        None.\n",
            " |  \n",
            " |  build_sf_fname(self, df)\n",
            " |      This method builds the file name for a given row in the State Farm DataFrame.\n",
            " |      \n",
            " |      Args:\n",
            " |        df (Pandas DataFrame): The State Farm DataFrame.\n",
            " |      \n",
            " |      Returns:\n",
            " |        None\n",
            " |  \n",
            " |  build_shoe_fname(self, start_path)\n",
            " |      This method builds the file name for a given directory.\n",
            " |      \n",
            " |      Args:\n",
            " |        start_path (str): The starting directory.\n",
            " |      \n",
            " |      Returns:\n",
            " |        DataFrame: The DataFrame containing the file names.\n",
            " |  \n",
            " |  check_spelling(self, df, col_dest='description')\n",
            " |      This method checks the spelling in a column and returns a new column \n",
            " |      \"misspelled\" and \"misspelled_count\".\n",
            " |      \n",
            " |      Args:\n",
            " |        df (DataFrame): The input DataFrame.\n",
            " |        col_dest (str): The column name to be checked, default \"description\"\n",
            " |      \n",
            " |      Returns:\n",
            " |        None\n",
            " |  \n",
            " |  clean_text(self, df)\n",
            " |      Cleans a text by replacing any special characters with a '_' character and \n",
            " |      replaces any bad word with a '***' character.\n",
            " |      It uses the _clean_text() and _clean_bad_word() helper functions.\n",
            " |      \n",
            " |      Args:\n",
            " |        df (Pandas DataFrame): The Pandas DataFrame containing the column 'tweet' \n",
            " |        which is to be cleaned.\n",
            " |      \n",
            " |      Returns:\n",
            " |        Pandas DataFrame: The modified Pandas DataFrame with a new column 'clean_tweet'.\n",
            " |  \n",
            " |  count_word(self, df, col_dest='description')\n",
            " |      This method counts the number of words in a column named \"wordc\"\n",
            " |      \n",
            " |      Args:\n",
            " |        df (DataFrame): The input DataFrame.\n",
            " |        col_dest (str): The column name to be counted, default \"description\"\n",
            " |      \n",
            " |      Returns:\n",
            " |        None\n",
            " |  \n",
            " |  draw_batch(self, df_filenames, disp_max=10, is_shuffle=False, figsize=(16, 8))\n",
            " |      This method draws the images specified in the DataFrame.\n",
            " |      \n",
            " |      Args:\n",
            " |        df_filenames (Pandas DataFrame): The DataFrame containing the file names.\n",
            " |        disp_max (int): The maximum number of images to be drawn. Default is 10.\n",
            " |        is_shuffle (bool): Whether to shuffle the images. Default is False.\n",
            " |        figsize (tuple): The figure size. Default is (16,8).\n",
            " |      \n",
            " |      Returns:\n",
            " |        None\n",
            " |  \n",
            " |  draw_text_null_data(self, df, color=(0.3, 0.36, 0.44))\n",
            " |      Draws a heatmap of missing values for a given Pandas DataFrame.\n",
            " |      \n",
            " |      Args:\n",
            " |        df (Pandas DataFrame): The data to draw a heatmap of from.\n",
            " |        color (tuple, optional): The color for the heatmap. Defaults to (0.3,0.36,0.44).\n",
            " |      \n",
            " |      Returns:\n",
            " |        None.\n",
            " |  \n",
            " |  draw_text_wordcloud(self, df_1column, xignore_words='cat', title='Word Cloud:')\n",
            " |      Draws a word cloud from a given column of text. It uses the helper function:\n",
            " |      _draw_image_wordcloud().\n",
            " |      \n",
            " |      Args:\n",
            " |        df_1column (Pandas DataFrame): The data to draw a heatmap of from.\n",
            " |        xignore_words (str, optional): Any word(s) that should be ignored. Defaults to 'cat'.\n",
            " |        title (str, optional): The title of the plot. Defaults to 'Word Cloud:'.\n",
            " |      \n",
            " |      Returns:\n",
            " |        None.\n",
            " |  \n",
            " |  draw_word_count(self, df, wc='wordc', is_stack_verticle=True)\n",
            " |      This method creates two plots:\n",
            " |        1. a boxplot of word count\n",
            " |        2. a histogram of word count\n",
            " |      \n",
            " |      Args:\n",
            " |        df (DataFrame): The input DataFrame.\n",
            " |        wc (str): The column name of word count, default \"wordc\".\n",
            " |        is_stack_verticle (bool): Whether to stack the two plots vertically. Default is True.\n",
            " |      \n",
            " |      Returns:\n",
            " |        None\n",
            " |  \n",
            " |  fetch_df(self, csv, sep=',')\n",
            " |      This method reads and loads a CSV file into a Pandas DataFrame.\n",
            " |      \n",
            " |      Args:\n",
            " |        csv (str): The path to a CSV file.\n",
            " |        sep (str): The column separator character, default is comma \",\".\n",
            " |      \n",
            " |      Returns:\n",
            " |        DataFrame: A pandas DataFrame.\n",
            " |  \n",
            " |  fetch_kaggle_comp_data(self, cname)\n",
            " |  \n",
            " |  fetch_kaggle_dataset(self, url, dest='kaggle')\n",
            " |  \n",
            " |  make_dir_dataframe(self, start_path)\n",
            " |      This method builds the file name for a given directory.\n",
            " |      \n",
            " |      Args:\n",
            " |        start_path (str): The starting directory.\n",
            " |      \n",
            " |      Returns:\n",
            " |        DataFrame: The DataFrame containing the file names.\n",
            " |  \n",
            " |  print_aug_char_random(self, df, action='insert', col_dest='description', bsize=3, aug_name='Augment')\n",
            " |      Performs random augmentation on the given text data and prints the results.\n",
            " |      \n",
            " |      Args:\n",
            " |        df (Pandas DataFrame): The Pandas DataFrame containing the column to be\n",
            " |          augmented.\n",
            " |        col_dest (str, optional): The column name of the column to be augmented.\n",
            " |          The default column name is 'description'.\n",
            " |        bsize (int, optional): The batch size of the data augmentation results to be\n",
            " |          printed. The default batch size is 3.\n",
            " |        aug_name (str, optional): The name of the data augmentation function. The\n",
            " |          default name is 'Augmented'.\n",
            " |        action (str, optional): The action of augmentation. Possible values include:\n",
            " |          - 'insert': Insert random characters in the target text.\n",
            " |          - 'delete': Delete random characters in the target text.\n",
            " |          - 'substitute': Substitute random characters in the target text.\n",
            " |          The default value is 'insert'.\n",
            " |      \n",
            " |      Returns:\n",
            " |        None.\n",
            " |  \n",
            " |  print_aug_keyboard(self, df, col_dest='description', bsize=3, aug_name='Keyboard Augment')\n",
            " |      Performs keyboard augmentation on the given text data and prints the results.\n",
            " |      \n",
            " |      Args:\n",
            " |        df (Pandas DataFrame): The Pandas DataFrame containing the column to be\n",
            " |          augmented.\n",
            " |        col_dest (str, optional): The column name of the column to be augmented.\n",
            " |          The default column name is 'description'.\n",
            " |        bsize (int, optional): The batch size of the data augmentation results to be\n",
            " |          printed. The default batch size is 3.\n",
            " |        aug_name (str, optional): The name of the data augmentation function. The\n",
            " |          default name is 'Keyboard Augment'.\n",
            " |      \n",
            " |      Returns:\n",
            " |        None.\n",
            " |  \n",
            " |  print_aug_ocr(self, df, col_dest='description', bsize=3, aug_name='Augmented')\n",
            " |      Performs OCR augmentation on the given text data and prints the results.\n",
            " |      \n",
            " |      Args:\n",
            " |        df (Pandas DataFrame): The Pandas DataFrame containing the column to be\n",
            " |          augmented.\n",
            " |        col_dest (str, optional): The column name of the column to be augmented.\n",
            " |          The default column name is 'description'.\n",
            " |        bsize (int, optional): The batch size of the data augmentation results to be\n",
            " |          printed. The default batch size is 3.\n",
            " |        aug_name (str, optional): The name of the data augmentation function. The\n",
            " |          default name is 'Augmented'.\n",
            " |      \n",
            " |      Returns:\n",
            " |        None.\n",
            " |  \n",
            " |  print_aug_word_antonym(self, df, col_dest='description', bsize=3, aug_name='Augment')\n",
            " |      Performs antonym augmentation on the given text data and prints the results.\n",
            " |      \n",
            " |      Args:\n",
            " |        df (Pandas DataFrame): The Pandas DataFrame containing the column to be\n",
            " |          augmented.\n",
            " |        col_dest (str, optional): The column name of the column to be augmented.\n",
            " |          The default column name is 'description'.\n",
            " |        bsize (int, optional): The batch size of the data augmentation results to be\n",
            " |          printed. The default batch size is 3.\n",
            " |        aug_name (str, optional): The name of the data augmentation function. The\n",
            " |          default name is 'Augmented'.\n",
            " |      \n",
            " |      Returns:\n",
            " |        None.\n",
            " |  \n",
            " |  print_aug_word_misspell(self, df, col_dest='description', bsize=3, aug_name='Augment')\n",
            " |      Performs misspell augmentation on the given text data and prints the results.\n",
            " |      \n",
            " |      Args:\n",
            " |        df (Pandas DataFrame): The Pandas DataFrame containing the column to be\n",
            " |          augmented.\n",
            " |        col_dest (str, optional): The column name of the column to be augmented.\n",
            " |          The default column name is 'description'.\n",
            " |        bsize (int, optional): The batch size of the data augmentation results to be\n",
            " |          printed. The default batch size is 3.\n",
            " |        aug_name (str, optional): The name of the data augmentation function. The\n",
            " |          default name is 'Augmented'.\n",
            " |      \n",
            " |      Returns:\n",
            " |        None.\n",
            " |  \n",
            " |  print_aug_word_random(self, df, action='swap', col_dest='description', bsize=3, aug_name='Augment')\n",
            " |      Performs random word augmentation on the given text data and prints the results.\n",
            " |      \n",
            " |      Args:\n",
            " |        df (Pandas DataFrame): The Pandas DataFrame containing the column to be\n",
            " |          augmented.\n",
            " |        col_dest (str, optional): The column name of the column to be augmented.\n",
            " |          The default column name is 'description'.\n",
            " |        bsize (int, optional): The batch size of the data augmentation results to be\n",
            " |          printed. The default batch size is 3.\n",
            " |        aug_name (str, optional): The name of the data augmentation function. The\n",
            " |          default name is 'Augmented'.\n",
            " |        action (str, optional): The action of augmentation. Possible values include:\n",
            " |          - 'swap': Swap two random words in the target text.\n",
            " |          - 'substitute': Substitute a random word with another random word from the\n",
            " |            vocab.\n",
            " |          - 'insert': Insert a random word at a random position in the target text.\n",
            " |          - 'delete': Delete a random word from the target text.\n",
            " |          The default value is 'swap'.\n",
            " |      \n",
            " |      Returns:\n",
            " |        None.\n",
            " |  \n",
            " |  print_aug_word_reserved(self, df, col_dest='description', reserved_tokens=None, bsize=3, aug_name='Augment')\n",
            " |      Performs reserved word augmentation on the given text data and prints the results.\n",
            " |      \n",
            " |      Args:\n",
            " |        df (Pandas DataFrame): The Pandas DataFrame containing the column to be\n",
            " |          augmented.\n",
            " |        col_dest (str, optional): The column name of the column to be augmented.\n",
            " |          The default column name is 'description'.\n",
            " |        reserved_tokens (list, optional): The list of words that will be reserved\n",
            " |          and not be augmented. The default value is None.\n",
            " |        bsize (int, optional): The batch size of the data augmentation results to be\n",
            " |          printed. The default batch size is 3.\n",
            " |        aug_name (str, optional): The name of the data augmentation function. The\n",
            " |          default name is 'Augmented'.\n",
            " |      \n",
            " |      Returns:\n",
            " |        None.\n",
            " |  \n",
            " |  print_aug_word_split(self, df, col_dest='description', bsize=3, aug_name='Augment')\n",
            " |      Performs split augmentation on the given text data and prints the results.\n",
            " |      \n",
            " |      Args:\n",
            " |        df (Pandas DataFrame): The Pandas DataFrame containing the column to be\n",
            " |          augmented.\n",
            " |        col_dest (str, optional): The column name of the column to be augmented.\n",
            " |          The default column name is 'description'.\n",
            " |        bsize (int, optional): The batch size of the data augmentation results to be\n",
            " |          printed. The default batch size is 3.\n",
            " |        aug_name (str, optional): The name of the data augmentation function. The\n",
            " |          default name is 'Augmented'.\n",
            " |      \n",
            " |      Returns:\n",
            " |        None.\n",
            " |  \n",
            " |  print_aug_word_synonym(self, df, col_dest='description', bsize=3, aug_name='Augment')\n",
            " |      Performs synonym augmentation on the given text data and prints the results.\n",
            " |      \n",
            " |      Args:\n",
            " |        df (Pandas DataFrame): The Pandas DataFrame containing the column to be\n",
            " |          augmented.\n",
            " |        col_dest (str, optional): The column name of the column to be augmented.\n",
            " |          The default column name is 'description'.\n",
            " |        bsize (int, optional): The batch size of the data augmentation results to be\n",
            " |          printed. The default batch size is 3.\n",
            " |        aug_name (str, optional): The name of the data augmentation function. The\n",
            " |          default name is 'Augmented'.\n",
            " |      \n",
            " |      Returns:\n",
            " |        None.\n",
            " |  \n",
            " |  print_batch_text(self, df_orig, disp_max=6, cols=['title', 'description'], is_larger_font=True)\n",
            " |      This method shows a batch of text data.\n",
            " |      \n",
            " |      Args:\n",
            " |        df_orig (DataFrame): The input DataFrame.\n",
            " |        disp_max (int): The maximum number of rows to be displayed. Default is 6.\n",
            " |        cols (list): The list of columns to display. Default is [\"title\", \"description\"].\n",
            " |        is_larger_font (bool): Whether to use larger font. Default is True.\n",
            " |      \n",
            " |      Returns:\n",
            " |        None\n",
            " |  \n",
            " |  remember_kaggle_access_keys(self, username, key)\n",
            " |      This method takes a username and a Kaggle API key as arguments and stores\n",
            " |      them in the class object.\n",
            " |      \n",
            " |      Args:\n",
            " |       username (str): The Kaggle username.\n",
            " |       key (str): The Kaggle API key.\n",
            " |      \n",
            " |      Returns:\n",
            " |       None\n",
            " |  \n",
            " |  say_sys_info(self)\n",
            " |      Print out system information. Useful for\n",
            " |      debugging purposes. Prints out information such as\n",
            " |      the system time, platform, Python version, PyTorch\n",
            " |      version, Pandas version, PIL version, and\n",
            " |      Matplotlib version. Also prints the number of CPU\n",
            " |      cores and the CPU speed.\n",
            " |      \n",
            " |      Note that this function is added to the class `PacktDataAug` via\n",
            " |      the decorator `@add_method()`. This means that you can\n",
            " |      call this function as `p.say_system_info()`,\n",
            " |      where `p` is an instance of `PacktDAtaAug`.\n",
            " |      \n",
            " |      Args:\n",
            " |        None\n",
            " |      \n",
            " |      Returns:\n",
            " |        None\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors defined here:\n",
            " |  \n",
            " |  __dict__\n",
            " |      dictionary for instance variables (if defined)\n",
            " |  \n",
            " |  __weakref__\n",
            " |      list of weak references to the object (if defined)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Push up all changes (Optional)"
      ],
      "metadata": {
        "id": "LEj7fgaIN9_S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- username: duchaba\n",
        "\n",
        "- password: [use the token]"
      ],
      "metadata": {
        "id": "lHXRf21BT9N8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import os\n",
        "# f = 'Data-Augmentation-with-Python'\n",
        "# os.chdir(f)\n",
        "# !git add -A\n",
        "# !git config --global user.email \"duc.haba@gmail.com\"\n",
        "# !git config --global user.name \"duchaba\"\n",
        "# !git commit -m \"end of session\"\n",
        "# # do the git push in the xterm console\n",
        "# #!git push"
      ],
      "metadata": {
        "id": "eSXJKJFlOEeE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %%script false --no-raise-error  #temporary stop execute for export file"
      ],
      "metadata": {
        "id": "JMJgBjC1Px5A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EzO7lDYDWeLz"
      },
      "source": [
        "# Summary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X2iUPf3EWePd"
      },
      "source": [
        "Every chaper will begin with same base class \"PacktDataAug\".\n",
        "\n",
        "‚úã FAIR WARNING:\n",
        "\n",
        "- The coding uses long and complete function path name.\n",
        "\n",
        "- Pluto wrote the code for easy to understand and not for compactness, fast execution, nor cleaverness.\n",
        "\n",
        "- Use Xterm to debug cloud server\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install colab-xterm\n",
        "# %load_ext colabxterm\n",
        "# %xterm"
      ],
      "metadata": {
        "id": "vXRG3KaeWbGx"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "gpuType": "A100",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}