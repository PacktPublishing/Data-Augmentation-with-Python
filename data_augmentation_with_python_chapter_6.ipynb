{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/duchaba/Data-Augmentation-with-Python/blob/main/data_augmentation_with_python_chapter_6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸŒ» Welcome to Chapter 6, Text Augmentation with Machine Learning\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "I am glad to see you using this Python Notebook. ðŸ¶\n",
        "\n",
        "The Python Notebook is an integral part of the book. You can add new â€œcode cellsâ€ to extend the functions, add your data, and explore new possibilities, such as downloading additional real-world datasets from the Kaggle website and coding the **Fun challenges**. Furthermore, the book has **Fun facts**, in-depth discussion about augmentation techniques, and Pluto, an imaginary Siberian Huskey coding companion. Together they will guide you every steps of the way.\n",
        "\n",
        "Pluto encourages you to copy or save a copy of this Python Notebook to your local space and add the â€œtext cellsâ€ to keep your notes. In other words, read the book and copy the relevant concept to this Python Notebookâ€™s text-cells. Thus, you can have the explanation, note, original code, your code, and any crazy future ideas in one place.  \n",
        "\n",
        "\n",
        "ðŸ’— I hope you enjoy reading the book and hacking code as much as I enjoy writing it.\n",
        "\n",
        "\n",
        "## ðŸŒŸ Amazon Book\n",
        "\n",
        "---\n",
        "\n",
        "- The book is available on the Amazon Book website:\n",
        "  - https://www.amazon.com/dp/1803246456\n",
        "\n",
        "  - Author: Duc Haba\n",
        "  - Published: 2023\n",
        "  - Page count: 390+\n",
        "\n",
        "\n",
        "- The original Python Notebook is on:\n",
        "  - https://github.com/PacktPublishing/Data-Augmentation-with-Python/blob/main/Chapter_6/data_augmentation_with_python_chapter_6.ipynb\n",
        "\n",
        "- ðŸš€ Click on the blue \"Open in Colab\" button at the top of this page to begin hacking.\n",
        "\n"
      ],
      "metadata": {
        "id": "SIt9Tm6eXARu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸ˜€ Excerpt from Chapter 6, Text Augmentation with Machine Learning\n",
        "\n",
        "---\n",
        "\n",
        "> In case you havenâ€™t bought the book. Here is an teaser from the first page of Chapter 6.\n",
        "\n",
        "---\n",
        "\n",
        "Text augmentation with machine learning (ML) is an advanced technique compared to the standard text augmenting methods we covered in the previous chapter. Ironically, text augmentation aims to improve ML model accuracy, but we used a pre-trained ML model to create additional training NLP data. Itâ€™s a circular process. ML coding is not in this bookâ€™s scope, but understanding the difference between using libraries and ML for text augmentation is beneficial.\n",
        "\n",
        "Augmentation libraries, whether for image, text, or audio, follow the traditional programming methodologies with structure data, loops, and conditional statements in the algorithm. For example, as shown in Chapter 5, the pseudocode for implementing the _print_aug_reserved() method could be as follows:\n",
        "\n",
        "```\n",
        "# define synonym words, pseudo-code\n",
        "\n",
        "reserved = [['happy', 'joyful', 'cheerful'],\n",
        "  ['sad', 'sorrowful', 'regretful']]\n",
        "\n",
        "# substitute the word with its synonym, pseudo-code\n",
        "\n",
        "for i, word in (input_text)\n",
        "\n",
        "  for set_word in (reserved)\n",
        "\n",
        "    for i, syn in set_word\n",
        "\n",
        "      if (syn == word)\n",
        "\n",
        "        input_text[i] = set_word[i+1]\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "The happy path code does not cover error checking, but the salient point is that the libraryâ€™s function follows the standard sequential coding method.\n",
        "\n",
        "On the other hand, ML is based on one of the 13 known ML algorithms, including deep learning (DL) (or artificial neural networks), Bidirectional Encoder Representations from Transformers (BERT), linear regression, random\n",
        "\n",
        "forest, Naive Bayes, and gradience boosting. The key to ML is that the system learns and not programs. DL uses the Universal Approximation theory, gradient descent, transfer learning, and hundreds of other techniques. ANNs have millions to billions of neural nodes â€“ for example, OpenAI GPT3 has 96 layers and 175 billion nodes. The central point is that ML has no familiarity with the _print_aug_reserved() pseudocode algorithm.\n",
        "\n",
        "The following is a representation of a DL architecture for image classification. It illustrates the difference between a procedural approach and the Neural Network algorithm. This figure was created from Latex and the Overleaf cloud system. The output is as follows:\n",
        "\n",
        "> ...skip...image...\n",
        "\n",
        "Figure 6.1 â€“ Representation of a DL model\n",
        "\n",
        "The Overleaf project and its code are from Mr. Duc Habaâ€™s public repository, and the URL is https://www.overleaf.com/project/6369a1eaba583e7cd423171b. You can clone and hack the code to display other AI models.\n",
        "\n",
        "This chapter will cover text augmentation with ML, and in particular, the following topics:\n",
        "\n",
        "- Machine learning models\n",
        "\n",
        "- Word augmenting\n",
        "\n",
        "- Sentence augmenting\n",
        "\n",
        "- Real-world NLP datasets\n",
        "\n",
        "- Reinforcing your learning through the Python Notebook\n",
        "\n",
        "Letâ€™s briefly describe the ML models used in the Python wrapper function code.\n",
        "\n",
        "---\n",
        "\n",
        "ðŸŒ´ *end of excerpt from the book*"
      ],
      "metadata": {
        "id": "ZZ8-JIXv6jmV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GitHub Clone"
      ],
      "metadata": {
        "id": "4b52gSlp60SN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim==4.2.0\n",
        "import gensim\n",
        "# print(gensim.__version__)\n",
        "# # version4.2.0"
      ],
      "metadata": {
        "id": "XIngCjpIiy9a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# git version should be 2.17.1 or higher\n",
        "!git --version"
      ],
      "metadata": {
        "id": "6oeDAu1u6zWf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "url = 'https://github.com/PacktPublishing/Data-Augmentation-with-Python'\n",
        "!git clone {url}"
      ],
      "metadata": {
        "id": "J9buwUR767Te"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fetch file from URL (Optional)\n",
        "\n",
        "- Uncommend the below 2 code cells if you want to use URL and not Git Clone"
      ],
      "metadata": {
        "id": "lOawA01L7Ok0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import requests\n",
        "# #\n",
        "# def fetch_file(url, dst):\n",
        "#   downloaded_obj = requests.get(url)\n",
        "#   with open(dst, \"wb\") as file:\n",
        "#     file.write(downloaded_obj.content)\n",
        "#   return"
      ],
      "metadata": {
        "id": "HsmQ7rgj67Ww"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# url = ''\n",
        "# dst = 'pluto_chapter_1.py'\n",
        "# fetch_file(url,dst)"
      ],
      "metadata": {
        "id": "_nKp0nxT67aH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run Pluto\n",
        "\n",
        "- Instantiate up Pluto, aka. \"Pluto, wake up!\""
      ],
      "metadata": {
        "id": "6462KTtr7cFQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# %% CARRY-OVER code install\n",
        "\n",
        "!pip install opendatasets --upgrade\n",
        "!pip install pyspellchecker\n",
        "!pip install missingno\n",
        "!pip install nltk\n",
        "!pip install wordcloud\n",
        "!pip install filter-profanity\n",
        "!pip install nlpaug"
      ],
      "metadata": {
        "id": "P10V_4JQ7DEl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#load and run the pluto chapter 1 Python code.\n",
        "pluto_file = 'Data-Augmentation-with-Python/pluto/pluto_chapter_5.py'\n",
        "%run {pluto_file}"
      ],
      "metadata": {
        "id": "u3zbkOO86_WB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Verify Pluto"
      ],
      "metadata": {
        "id": "yhRR0JSf746h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pluto.say_sys_info()"
      ],
      "metadata": {
        "id": "nBIw7fiI6_Zy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## (Optional) Export to .py"
      ],
      "metadata": {
        "id": "7jduoUhMCN3H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pluto_chapter_6 = 'Data-Augmentation-with-Python/pluto/pluto_chapter_6.py'\n",
        "!cp {pluto_file} {pluto_chapter_6}"
      ],
      "metadata": {
        "id": "Gfx9Ai5Opyqp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# âœ‹ Get Kaggle username and api key (Optional for this chapter)\n",
        "\n",
        "- Install the following libraries, and import it on the Notebook.\n",
        "- Follow by initialize Kaggle username, key and fetch methods.\n",
        "- STOP: Update your Kaggle access username or key first."
      ],
      "metadata": {
        "id": "kQ6ap39x8HyF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------- : --------------------\n",
        "# READ ME\n",
        "# Chapter 2 begin:\n",
        "# Install the following libraries, and import it on the Notebook.\n",
        "# Follow by initialize Kaggle username, key and fetch methods.\n",
        "# STOP: Update your Kaggle access username or key first.\n",
        "# -------------------- : --------------------\n",
        "\n",
        "!pip install opendatasets --upgrade\n",
        "import opendatasets\n",
        "print(\"\\nrequired version 0.1.22 or higher: \", opendatasets.__version__)\n",
        "\n",
        "!pip install pyspellchecker\n",
        "import spellchecker\n",
        "print(\"\\nRequired version 0.7+\", spellchecker.__version__)\n",
        "\n",
        "# STOP: Update your Kaggle access username or key first.\n",
        "pluto.remember_kaggle_access_keys(\"YOUR-USERNAME\", \"YOUR-KEY\")\n",
        "pluto._write_kaggle_credit()\n",
        "import kaggle\n",
        "\n",
        "@add_method(PacktDataAug)\n",
        "def fetch_kaggle_comp_data(self,cname):\n",
        "  #self._write_kaggle_credit()  # need to run only once.\n",
        "  path = pathlib.Path(cname)\n",
        "  kaggle.api.competition_download_cli(str(path))\n",
        "  zipfile.ZipFile(f'{path}.zip').extractall(path)\n",
        "  return\n",
        "\n",
        "@add_method(PacktDataAug)\n",
        "def fetch_kaggle_dataset(self,url,dest=\"kaggle\"):\n",
        "  #self._write_kaggle_credit()    # need to run only once.\n",
        "  opendatasets.download(url,data_dir=dest)\n",
        "  return\n",
        "# -------------------- : --------------------\n"
      ],
      "metadata": {
        "id": "sf4Obdp-77CL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fetch data from chapter 5"
      ],
      "metadata": {
        "id": "gFAQHzXjYvr9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "f = 'Data-Augmentation-with-Python/pluto_data'\n",
        "!ls -la {f}"
      ],
      "metadata": {
        "id": "j-fk5IEEyfUG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f = 'Data-Augmentation-with-Python/pluto_data/netflix_data.csv'\n",
        "pluto.df_netflix_data = pluto.fetch_df(f,sep='~')"
      ],
      "metadata": {
        "id": "Asm98OsWXcvc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f = 'Data-Augmentation-with-Python/pluto_data/twitter_data.csv'\n",
        "pluto.df_twitter_data = pluto.fetch_df(f,sep='~')"
      ],
      "metadata": {
        "id": "5Xt1GrQXat_I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pluto.print_batch_text(pluto.df_netflix_data, cols=['title','description'])"
      ],
      "metadata": {
        "id": "Se_a_tFhXcyM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pluto.draw_text_wordcloud(pluto.df_netflix_data.description,\n",
        "  xignore_words=wordcloud.STOPWORDS,\n",
        "  title='Word Cloud: Netflix Movie Review')"
      ],
      "metadata": {
        "id": "9quCB-ddXc1b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pluto.draw_text_wordcloud(pluto.df_twitter_data.clean_tweet,\n",
        "  xignore_words=wordcloud.STOPWORDS,\n",
        "  title='Word Cloud: Twitter Tweets')"
      ],
      "metadata": {
        "id": "dWE1Gwb4Xc4k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extened control text"
      ],
      "metadata": {
        "id": "YodrXsBjVH62"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# %%writefile -a {pluto_chapter_6}\n",
        "\n",
        "pluto.version = 7.0\n",
        "#\n",
        "pluto.orig_text = '''It was the best of times. It was the worst of times. It was the age of wisdom. It was the age of foolishness. It was the epoch of belief. It was the epoch of incredulity.'''\n",
        "pluto.orig_dickens_page = '''There were a king with a large jaw and a queen with a plain face,\n",
        "on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France.\n",
        "In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes,\n",
        "that things in general were settled for ever.\n",
        "It was the year of Our Lord one thousand seven hundred and seventy-five.\n",
        "Spiritual revelations were conceded to England at that favoured period,\n",
        "as at this. Mrs. Southcott had recently attained her five-and-twentieth blessed birthday,\n",
        "of whom a prophetic private in the Life Guards had heralded the sublime appearance\n",
        "by announcing that arrangements were made for the swallowing up of London and Westminster.\n",
        "Even the Cock-lane ghost had been laid only a round dozen of years,\n",
        "after rapping out its messages,\n",
        "as the spirits of this very year last past (supernaturally deficient in originality) rapped out theirs.\n",
        "Mere messages in the earthly order of events had lately come to the English Crown and People,\n",
        "from a congress of British subjects in America: which, strange to relate,\n",
        "have proved more important to the human race than any communications yet received through any of the chickens of the Cock-lane brood.'''\n",
        "\n",
        "pluto.orig_melville_page2 = '''Call me Ishmael.\n",
        "Some years agoâ€”never mind how long preciselyâ€”having little or no money in my purse,\n",
        "and nothing particular to interest me on shore,\n",
        "I thought I would sail about a little and see the watery part of the world.\n",
        "It is a way I have of driving off the spleen and regulating the circulation.'''\n",
        "\n",
        "pluto.orig_melville_page = '''Call me Ishmael.\n",
        "Some years agoâ€”never mind how long preciselyâ€”having little or no money in my purse,\n",
        "and nothing particular to interest me on shore,\n",
        "I thought I would sail about a little and see the watery part of the world.\n",
        "It is a way I have of driving off the spleen and regulating the circulation.\n",
        "Whenever I find myself growing grim about the mouth; whenever it is a damp,\n",
        "drizzly November in my soul;\n",
        "whenever I find myself involuntarily pausing before coffin warehouses,\n",
        "and bringing up the rear of every funeral I meet;\n",
        "and especially whenever my hypos get such an upper hand of me,\n",
        "that it requires a strong moral principle to prevent me from deliberately stepping into the street,\n",
        "and methodically knocking peopleâ€™s hats offâ€”then,\n",
        "I account it high time to get to sea as soon as I can.\n",
        "This is my substitute for pistol and ball.\n",
        "With a philosophical flourish Cato throws himself upon his sword;\n",
        "I quietly take to the ship. There is nothing surprising in this.\n",
        "If they but knew it, almost all men in their degree, some time or other,\n",
        "cherish very nearly the same feelings towards the ocean with me.'''\n",
        "\n",
        "#\n",
        "pluto.orig_carroll_page = '''Alice was beginning to get very tired of sitting by her sister on the bank,\n",
        "and of having nothing to do.\n",
        "Once or twice she had peeped into the book her sister was reading,\n",
        "but it had no pictures or conversations in it.\n",
        "â€œand what is the use of a book,â€ thought Alice â€œwithout pictures or conversations?â€\n",
        "So she was considering in her own mind. as well as she could,\n",
        "for the hot day made her feel very sleepy and stupid.\n",
        "whether the pleasure of making a daisy-chain would be worth the trouble of getting up and picking the daisies,\n",
        "when suddenly a White Rabbit with pink eyes ran close by her.\n",
        "There was nothing so very remarkable in that;\n",
        "nor did Alice think it so very much out of the way to hear the Rabbit say to itself.\n",
        "â€œOh dear! Oh dear! I shall be late!â€\n",
        "when she thought it over afterwards,\n",
        "it occurred to her that she ought to have wondered at this,\n",
        "but at the time it all seemed quite natural;\n",
        "but when the Rabbit actually took a watch out of its waistcoat-pocket,\n",
        "and looked at it, and then hurried on, Alice started to her feet,\n",
        "for it flashed across her mind that she had never before seen a rabbit with either a waistcoat-pocket,\n",
        "or a watch to take out of it,\n",
        "and burning with curiosity,\n",
        "she ran across the field after it,\n",
        "and fortunately was just in time to see it pop down a large rabbit-hole under the hedge.\n",
        "'''\n",
        "#\n",
        "pluto.orig_carroll_page2 = '''Alice was beginning to get very tired of sitting by her sister on the bank,\n",
        "and of having nothing to do.\n",
        "Once or twice she had peeped into the book her sister was reading,\n",
        "but it had no pictures or conversations in it.\n",
        "â€œand what is the use of a book,â€ thought Alice â€œwithout pictures or conversations?â€\n",
        "So she was considering in her own mind. as well as she could,\n",
        "for the hot day made her feel very sleepy and stupid.\n",
        "whether the pleasure of making a daisy-chain would be worth the trouble of getting up and picking the daisies,\n",
        "when suddenly a White Rabbit with pink eyes ran close by her.'''\n",
        "#\n",
        "pluto.orig_self = '''Text augmentation with Machine Learning (ML) is an advanced technique.\n",
        "Ironically, Text augmentation aims to improve ML model accuracy,\n",
        "but we used a pre-trained ML model to create additional training NLP data.\n",
        "Itâ€™s a circular process. ML coding is not in this bookâ€™s scope,\n",
        "but understanding the difference between text augmentation using libraries and ML is beneficial.\n",
        "Augmentation libraries, whether for image, text, or audio,\n",
        "follow the traditional programming methodologies with structure data, loops,\n",
        "and conditional statements in the algorithm.\n",
        "For example, from Chapter 5, the pseudo-code for implementing the method _print_aug_reserved() could be as follows:'''"
      ],
      "metadata": {
        "id": "LBiRho15VxTr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %%writefile -a {pluto_chapter_6}\n",
        "\n",
        "# prompt: write detail Python documentation with default value for the following function: _clean_text\n",
        "@add_method(PacktDataAug)\n",
        "def _clean_text(self,x):\n",
        "  \"\"\"Clean text by removing all non-alphanumeric characters.\n",
        "\n",
        "  Args:\n",
        "    x (str): Input text.\n",
        "\n",
        "  Returns:\n",
        "    str: Cleaned text.\n",
        "  \"\"\"\n",
        "  return (re.sub('[^A-Za-z0-9 .,!?#@]+', '', str(x)))"
      ],
      "metadata": {
        "id": "mgQDAvNGmlPB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pluto.df_netflix_data['description'] = pluto.df_netflix_data['description'].apply(pluto._clean_text)"
      ],
      "metadata": {
        "id": "LIeAnM3rmVB-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fO57mq6DOvZW"
      },
      "source": [
        "# Word2Vec"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !ls -la ../model/"
      ],
      "metadata": {
        "id": "oK-3H-GCRaRV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %%writefile -a {pluto_chapter_6}\n",
        "\n",
        "from nlpaug.util.file.download import DownloadUtil\n",
        "\n",
        "#DownloadUtil.download_word2vec(dest_dir='.') # word2vec\n",
        "DownloadUtil.download_glove(model_name='glove.6B', dest_dir='.') # GloVe\n",
        "DownloadUtil.download_fasttext(model_name='wiki-news-300d-1M', dest_dir='.') # fasttext model\n"
      ],
      "metadata": {
        "id": "jnSCgVH5WzxH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %%writefile -a {pluto_chapter_6}\n",
        "\n",
        "try:\n",
        "  DownloadUtil.download_word2vec(dest_dir='.') # word2vec\n",
        "except Exception:\n",
        "  print('\\nIt happen frequently that this file can not be download due to too many access.')\n",
        "  print('When it failed, you can not use Word2Vec transformation/augmentation.')\n",
        "  print('You could download the file: GoogleNews-vectors-negative300.bin.gz')\n",
        "  print('From URL: https://drive.google.com/uc?export=download&id=0B7XkCwpI5KDYNlNUTTlSS21pQmM \\n')"
      ],
      "metadata": {
        "id": "O4vC_3iKLAOW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %%writefile -a {pluto_chapter_6}\n",
        "\n",
        "# prompt: write detail Python documentation with default value for the following function: print_aug_ai_word2vec()\n",
        "import nlpaug\n",
        "@add_method(PacktDataAug)\n",
        "def print_aug_ai_word2vec(self, df,\n",
        "  col_dest=\"description\",\n",
        "  action='insert',\n",
        "  model_type='word2vec',\n",
        "  model_path='GoogleNews-vectors-negative300.bin',\n",
        "  bsize=3,\n",
        "  aug_name='Augment',\n",
        "  is_orig_control=True):\n",
        "\n",
        "  \"\"\"Augment text using Word2Vec model.\n",
        "\n",
        "  Args:\n",
        "    df (pd.DataFrame): Input pandas DataFrame.\n",
        "    col_dest (str): Column name to store augmented text. Default is 'description'.\n",
        "    action (str): Augmentation action. Possible values:\n",
        "      - 'insert': insert augmented text into the column. Default is 'insert'.\n",
        "      - 'replace': replace original text with the augmented text.\n",
        "    model_type (str): Word embedding model type. Possible values:\n",
        "      - 'word2vec': Word2Vec (default)\n",
        "      - 'glove': GloVe\n",
        "      - 'fasttext': FastText\n",
        "    model_path (str): Path to the pre-trained word embedding model.\n",
        "      Default is 'GoogleNews-vectors-negative300.bin'.\n",
        "    bsize (int): Batch size for augmentation. Default is 3.\n",
        "    aug_name (str): Name of the augmentation. Default is 'Augment'.\n",
        "    is_orig_control (bool): Whether to control the original text. Default is True.\n",
        "\n",
        "  Returns:\n",
        "    pd.DataFrame: Augmented pandas DataFrame.\n",
        "  \"\"\"\n",
        "  aug_func = nlpaug.augmenter.word.WordEmbsAug(model_type=model_type,action=action,model_path=model_path)\n",
        "  self._print_aug_batch(df, aug_func,col_dest=col_dest,bsize=bsize, aug_name=aug_name)\n",
        "  return"
      ],
      "metadata": {
        "id": "IyIftW1iOqJN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pluto.print_aug_ai_word2vec(pluto.df_netflix_data,\n",
        "  col_dest='description',\n",
        "  action='insert',\n",
        "  aug_name='Word2Vec-GoogleNews Word Embedding Augment')"
      ],
      "metadata": {
        "id": "QJWmEr9CEvkH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pluto.print_aug_ai_word2vec(pluto.df_netflix_data,\n",
        "  col_dest='description',\n",
        "  action='insert',\n",
        "  aug_name='Word2Vec-GoogleNews Word Embedding Augment')"
      ],
      "metadata": {
        "id": "bpdpR53_OqMn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pluto.print_aug_ai_word2vec(pluto.df_twitter_data,col_dest='clean_tweet',action='insert',aug_name='Word2Vec-GoogleNews Word Embedding Augment')"
      ],
      "metadata": {
        "id": "BDMnyi0aOqPq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Substitute"
      ],
      "metadata": {
        "id": "2lIhZzkeS3f1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pluto.print_aug_ai_word2vec(pluto.df_netflix_data,\n",
        "  col_dest='description',\n",
        "  action='substitute',\n",
        "  aug_name='Word2Vec-GoogleNews Word Embedding Augment')"
      ],
      "metadata": {
        "id": "rFi1CQDvS6Vk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pluto.print_aug_ai_word2vec(pluto.df_twitter_data,\n",
        "  col_dest='clean_tweet',\n",
        "  action='substitute',\n",
        "  aug_name='Word2Vec-GoogleNews Word Embedding Augment')"
      ],
      "metadata": {
        "id": "QPJKjPGAS6Yv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_page = pandas.DataFrame([pluto.orig_carroll_page], columns=['page'])\n",
        "pluto.print_aug_ai_word2vec(df_page,\n",
        "  col_dest='page',\n",
        "  action='substitute',\n",
        "  aug_name='Word2Vec-GoogleNews Word Embedding Augment',\n",
        "  bsize=1,\n",
        "  is_orig_control=False)"
      ],
      "metadata": {
        "id": "CH4qoUd9qkoh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_page = pandas.DataFrame([pluto.orig_dickens_page], columns=['page'])\n",
        "pluto.print_aug_ai_word2vec(df_page,\n",
        "  col_dest='page',\n",
        "  action='substitute',\n",
        "  aug_name='Word2Vec-GoogleNews Word Embedding Augment',\n",
        "  bsize=1,\n",
        "  is_orig_control=False)"
      ],
      "metadata": {
        "id": "_R3uu_gJS6cA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ert6W8iPOvZa"
      },
      "source": [
        "#BERT"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "gKfFXMpAsQ7U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %%writefile -a {pluto_chapter_6}\n",
        "\n",
        "import transformers\n",
        "print(transformers.__version__)"
      ],
      "metadata": {
        "id": "sTonbBjLGlvA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install simpletransformers>=0.61.10\n",
        "import simpletransformers\n",
        "#print(simpletransformers.__version__)"
      ],
      "metadata": {
        "id": "hmfLVK_lG1ct"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk>=3.4.5\n",
        "!pip install gensim>=4.1.2"
      ],
      "metadata": {
        "id": "YfujVAx3G6Ul"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %%writefile -a {pluto_chapter_6}\n",
        "\n",
        "# prompt: write detail Python documentation with default value for the following function: print_aug_ai_bert()\n",
        "import nlpaug\n",
        "@add_method(PacktDataAug)\n",
        "def print_aug_ai_bert(self, df,\n",
        "  col_dest=\"description\",\n",
        "  action='insert',\n",
        "  model_path='bert-base-uncased',\n",
        "  bsize=3,\n",
        "  aug_name='Augment',\n",
        "  is_orig_control=True):\n",
        "\n",
        "  \"\"\"\n",
        "  Augment text using BERT model.\n",
        "\n",
        "  Args:\n",
        "    df (pd.DataFrame): Input pandas DataFrame.\n",
        "    col_dest (str): Column name to store augmented text. Default is 'description'.\n",
        "    action (str): Augmentation action. Possible values:\n",
        "      - 'insert': insert augmented text into the column. Default is 'insert'.\n",
        "      - 'replace': replace original text with the augmented text.\n",
        "    model_path (str): Path to the pre-trained BERT model.\n",
        "      Default is 'bert-base-uncased'.\n",
        "    bsize (int): Batch size for augmentation. Default is 3.\n",
        "    aug_name (str): Name of the augmentation. Default is 'Augment'.\n",
        "    is_orig_control (bool): Whether to control the original text. Default is True.\n",
        "\n",
        "  Returns:\n",
        "    pd.DataFrame: Augmented pandas DataFrame.\n",
        "  \"\"\"\n",
        "  aug_func = nlpaug.augmenter.word.ContextualWordEmbsAug(action=action,model_path=model_path)\n",
        "  #self._print_aug_batch(df, aug_func,col_dest=col_dest,bsize=bsize, aug_name=aug_name,is_orig_control=is_orig_control)\n",
        "  self._print_aug_batch(df, aug_func,col_dest=col_dest,bsize=bsize, aug_name=aug_name)\n",
        "  return"
      ],
      "metadata": {
        "id": "-02N5CaMULnq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pluto.print_aug_ai_bert(pluto.df_netflix_data,col_dest='description',\n",
        "  action='insert',aug_name='BERT Embedding Insert Augment')"
      ],
      "metadata": {
        "id": "dZdWtGcYT_jV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pluto.print_aug_ai_bert(pluto.df_twitter_data,col_dest='clean_tweet',\n",
        "  action='insert',aug_name='BERT Embedding Insert Augment')"
      ],
      "metadata": {
        "id": "EV7EV6OoT_tp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Substitute"
      ],
      "metadata": {
        "id": "VeSjdgRSVLXr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pluto.print_aug_ai_bert(pluto.df_netflix_data,col_dest='description',\n",
        "  action='substitute',aug_name='BERT Embedding Substitute Augment')"
      ],
      "metadata": {
        "id": "v6QDq9oiVTMa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pluto.print_aug_ai_bert(pluto.df_twitter_data,col_dest='clean_tweet',\n",
        "  action='substitute',aug_name='BERT Embedding Substitute Augment')"
      ],
      "metadata": {
        "id": "r8ZvWEySVTPk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "30ywh0jWOvZb"
      },
      "outputs": [],
      "source": [
        "df_page = pandas.DataFrame([pluto.orig_dickens_page], columns=['page'])\n",
        "pluto.print_aug_ai_bert(df_page,\n",
        "  col_dest='page',\n",
        "  action='substitute',\n",
        "  aug_name='BERT Embedding Substitute Augment',\n",
        "  bsize=1,\n",
        "  is_orig_control=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EgIO-PlOOvZc"
      },
      "outputs": [],
      "source": [
        "df_page = pandas.DataFrame([pluto.orig_melville_page], columns=['page'])\n",
        "pluto.print_aug_ai_bert(df_page,\n",
        "  col_dest='page',\n",
        "  action='substitute',\n",
        "  aug_name='BERT Embedding Substitute Augment',\n",
        "  bsize=1,\n",
        "  is_orig_control=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RoBERTa"
      ],
      "metadata": {
        "id": "u996W_0AVuXp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pluto.print_aug_ai_bert(pluto.df_netflix_data,col_dest='description',\n",
        "  model_path='roberta-base',\n",
        "  action='insert',aug_name='Roberta Embedding Insert Augment')"
      ],
      "metadata": {
        "id": "QJEnGQC0V7Ri"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pluto.print_aug_ai_bert(pluto.df_twitter_data,col_dest='clean_tweet',\n",
        "  model_path='roberta-base',\n",
        "  action='insert',aug_name='Roberta Embedding Insert Augment')"
      ],
      "metadata": {
        "id": "eiQ68dyJV7Uz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Substitute"
      ],
      "metadata": {
        "id": "WPhvZqtAXStc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pluto.print_aug_ai_bert(pluto.df_netflix_data,col_dest='description',\n",
        "  model_path='roberta-base',\n",
        "  action='substitute',aug_name='Roberta Embedding Substitute Augment')"
      ],
      "metadata": {
        "id": "aSHAE7AaV7X7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pluto.print_aug_ai_bert(pluto.df_twitter_data,col_dest='clean_tweet',\n",
        "  model_path='roberta-base',\n",
        "  action='substitute',aug_name='Roberta Embedding Substitute Augment')"
      ],
      "metadata": {
        "id": "9W-_ZidFV7bH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_page = pandas.DataFrame([pluto.orig_dickens_page], columns=['page'])\n",
        "pluto.print_aug_ai_bert(df_page,\n",
        "  col_dest='page',\n",
        "  model_path='roberta-base',\n",
        "  action='substitute',\n",
        "  aug_name='Roberta Embedding Substitute Augment',\n",
        "  bsize=1,\n",
        "  is_orig_control=False)"
      ],
      "metadata": {
        "id": "twdvZPN2u4BN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YBf4gqwmOvZc"
      },
      "outputs": [],
      "source": [
        "df_page = pandas.DataFrame([pluto.orig_melville_page], columns=['page'])\n",
        "pluto.print_aug_ai_bert(df_page,\n",
        "  col_dest='page',\n",
        "  model_path='roberta-base',\n",
        "  action='substitute',\n",
        "  aug_name='Roberta Embedding Substitute Augment',\n",
        "  bsize=1,\n",
        "  is_orig_control=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QRtJ2z3_OvZj"
      },
      "source": [
        "# Back Translation"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sacremoses"
      ],
      "metadata": {
        "id": "bfOwoGtJvkov"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %%writefile -a {pluto_chapter_6}\n",
        "\n",
        "# prompt: write detail Python documentation with default value for the following function: print_aug_ai_back_translation()\n",
        "@add_method(PacktDataAug)\n",
        "def print_aug_ai_back_translation(self, df, col_dest=\"description\",\n",
        "  from_model_name='facebook/wmt19-en-de',\n",
        "  to_model_name='facebook/wmt19-de-en',\n",
        "  bsize=3, aug_name='Augment',\n",
        "  is_orig_control=True):\n",
        "\n",
        "  \"\"\"\n",
        "  Augment text using back translation.\n",
        "\n",
        "  Args:\n",
        "      df (pd.DataFrame): Input pandas DataFrame.\n",
        "      col_dest (str): Column name to store augmented text. Default is 'description'.\n",
        "      from_model_name (str): Pre-trained model name for the first language.\n",
        "        Default is 'facebook/wmt19-en-de'.\n",
        "      to_model_name (str): Pre-trained model name for the second language.\n",
        "        Default is 'facebook/wmt19-de-en'.\n",
        "      bsize (int): Batch size for augmentation. Default is 3.\n",
        "      aug_name (str): Name of the augmentation. Default is 'Augment'.\n",
        "      is_orig_control (bool): Whether to control the original text. Default is True.\n",
        "\n",
        "  Returns:\n",
        "    pd.DataFrame: Augmented pandas DataFrame.\n",
        "  \"\"\"\n",
        "\n",
        "  aug_func = nlpaug.augmenter.word.BackTranslationAug(from_model_name=from_model_name,\n",
        "    to_model_name=to_model_name)\n",
        "  self._print_aug_batch(df, aug_func,col_dest=col_dest,bsize=bsize, aug_name=aug_name)\n",
        "  return"
      ],
      "metadata": {
        "id": "2peyD4nxXytF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pluto.print_aug_ai_back_translation(pluto.df_netflix_data,col_dest='description',\n",
        "  from_model_name='facebook/wmt19-en-de',\n",
        "  to_model_name='facebook/wmt19-de-en',\n",
        "  aug_name='FaceBook Back Translation: English <-> German Augment')"
      ],
      "metadata": {
        "id": "TvQSRxQbXy8y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# https://huggingface.co/models"
      ],
      "metadata": {
        "id": "ya0J5nTybqc2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pluto.print_aug_ai_back_translation(pluto.df_twitter_data,col_dest='clean_tweet',\n",
        "  from_model_name='facebook/wmt19-en-de',\n",
        "  to_model_name='facebook/wmt19-de-en',\n",
        "  aug_name='FaceBook Back Translation: English <-> German Augment')"
      ],
      "metadata": {
        "id": "gR1AitbqXzBB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pluto.print_aug_ai_back_translation(pluto.df_netflix_data,col_dest='description',\n",
        "  from_model_name='facebook/wmt19-en-de',\n",
        "  to_model_name='facebook/wmt19-de-en',\n",
        "  aug_name='FaceBook Back Translation: English <-> German Augment')"
      ],
      "metadata": {
        "id": "2InysGvoyBTn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Russian"
      ],
      "metadata": {
        "id": "hbtGGU2__i3O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pluto.print_aug_ai_back_translation(pluto.df_netflix_data,col_dest='description',\n",
        "  from_model_name='facebook/wmt19-en-ru',\n",
        "  to_model_name='facebook/wmt19-ru-en',\n",
        "  aug_name='FaceBook Back Translation: English <-> Russian Augment')"
      ],
      "metadata": {
        "id": "T3GfAXRIaA4V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pluto.print_aug_ai_back_translation(pluto.df_twitter_data,col_dest='clean_tweet',\n",
        "  from_model_name='facebook/wmt19-en-ru',\n",
        "  to_model_name='facebook/wmt19-ru-en',\n",
        "  aug_name='FaceBook Back Translation: English <-> Russian Augment')"
      ],
      "metadata": {
        "id": "9o8KPJN0aA8F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DshLBm_UOvZn"
      },
      "source": [
        "# T5-Base"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %%writefile -a {pluto_chapter_6}\n",
        "\n",
        "# prompt: write detail Python documentation with default value for the following function: _fetch_larger_font()\n",
        "@add_method(PacktDataAug)\n",
        "def _fetch_larger_font(self):\n",
        "\n",
        "  \"\"\"\n",
        "  Fetch larger font properties.\n",
        "\n",
        "  Args:\n",
        "    None.\n",
        "\n",
        "  Returns:\n",
        "    list: List of font properties.\n",
        "  \"\"\"\n",
        "\n",
        "  heading_properties = [('font-size', '20px')]\n",
        "  cell_properties = [('font-size', '18px'), ('text-align', 'left')]\n",
        "  dfstyle = [dict(selector=\"th\", props=heading_properties),\n",
        "    dict(selector=\"td\", props=cell_properties)]\n",
        "  return dfstyle"
      ],
      "metadata": {
        "id": "PWYurl0o-jAt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %%writefile -a {pluto_chapter_6}\n",
        "\n",
        "# prompt: write detail Python documentation with default value for the following function: _print_aug_ai()\n",
        "@add_method(PacktDataAug)\n",
        "def _print_aug_ai(self, orig, aug_func,\n",
        "  bsize=2, aug_name='Augmented',is_larger_font=True):\n",
        "\n",
        "  \"\"\"\n",
        "  Print augmented text.\n",
        "\n",
        "  Args:\n",
        "      orig (str): Original text.\n",
        "      aug_func (nlpaug.Augmenter): Augmenter.\n",
        "      bsize (int): Batch size for augmentation. Default is 2.\n",
        "      aug_name (str): Name of the augmentation. Default is 'Augmented'.\n",
        "      is_larger_font (bool): Whether to use larger font. Default is True.\n",
        "\n",
        "  Returns:\n",
        "    pd.DataFrame: Augmented pandas DataFrame.\n",
        "  \"\"\"\n",
        "  aug = aug_func.augment(orig)\n",
        "  data = [[aug[0]]]\n",
        "  df_aug = pandas.DataFrame(data, columns=[aug_name])\n",
        "  #\n",
        "  if (bsize > 1):\n",
        "    for i in range(bsize-1):\n",
        "      aug = aug_func.augment(orig)\n",
        "      data = [[aug[0]]]\n",
        "      t = pandas.DataFrame(data, columns=[aug_name])\n",
        "      df_aug = df_aug.append(t, ignore_index=True)\n",
        "  #\n",
        "  with pandas.option_context(\"display.max_colwidth\", None):\n",
        "    if (is_larger_font):\n",
        "      # df_aug.style.set_properties(**{'text-align': 'left'})\n",
        "      # display(df_aug.style.set_properties(**{'text-align': 'left'}))\n",
        "      display(df_aug.style.set_table_styles(self._fetch_larger_font()))\n",
        "    else:\n",
        "      display(df_aug)\n",
        "  return df_aug\n",
        "#\n",
        "# prompt: write detail Python documentation with default value for the following function: print_aug_ai_t5()\n",
        "@add_method(PacktDataAug)\n",
        "def print_aug_ai_t5(self, orig,\n",
        "  bsize=2,\n",
        "  aug_name='T5_summary',\n",
        "  is_orig_control=True):\n",
        "\n",
        "  \"\"\"\n",
        "  Augment text using T5 for text summarization.\n",
        "\n",
        "  Args:\n",
        "      orig (str): Original text.\n",
        "      bsize (int): Batch size for augmentation. Default is 2.\n",
        "      aug_name (str): Name of the augmentation. Default is 'T5_summary'.\n",
        "      is_orig_control (bool): Whether to control the original text. Default is True.\n",
        "\n",
        "  Returns:\n",
        "    pd.DataFrame: Augmented pandas DataFrame.\n",
        "  \"\"\"\n",
        "\n",
        "  aug_func = nlpaug.augmenter.sentence.AbstSummAug(model_path='t5-base')\n",
        "  df = self._print_aug_ai(orig, aug_func,bsize=bsize, aug_name=aug_name)\n",
        "  return df"
      ],
      "metadata": {
        "id": "xT_x7vmz1Wfl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pluto.df_t5_carroll = pluto.print_aug_ai_t5(pluto.orig_carroll_page, bsize=1)"
      ],
      "metadata": {
        "id": "JDwH5bq36Ln-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pluto.df_t5_carroll = pluto.print_aug_ai_t5(pluto.orig_carroll_page2, bsize=1)"
      ],
      "metadata": {
        "id": "LEhRXqR-X_fO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pluto.df_t5_melville = pluto.print_aug_ai_t5(pluto.orig_melville_page, bsize=1)"
      ],
      "metadata": {
        "id": "r1oK3bhX_fyB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pluto.df_t5_dickens = pluto.print_aug_ai_t5(pluto.orig_dickens_page, bsize=1)"
      ],
      "metadata": {
        "id": "mKxdKl75_f1b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pluto.df_t5_self = pluto.print_aug_ai_t5(pluto.orig_self, bsize=1)"
      ],
      "metadata": {
        "id": "1K-FP4Vy_f4x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sentense flow"
      ],
      "metadata": {
        "id": "zC3Gsc8QyC55"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# %%writefile -a {pluto_chapter_6}\n",
        "\n",
        "import nlpaug.augmenter.word\n",
        "import nlpaug.augmenter.sentence\n",
        "import nlpaug.flow\n",
        "pluto.ai_aug_glove = nlpaug.augmenter.word.WordEmbsAug(\n",
        "    model_type='glove', model_path='glove.6B.300d.txt',\n",
        "    action=\"substitute\")\n",
        "#\n",
        "pluto.ai_aug_glove.aug_p=0.5\n",
        "#\n",
        "pluto.ai_aug_bert = nlpaug.augmenter.word.ContextualWordEmbsAug(\n",
        "    model_path='bert-base-uncased',\n",
        "    action='substitute', top_k=20)\n"
      ],
      "metadata": {
        "id": "96LvpOPdNeeH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %%writefile -a {pluto_chapter_6}\n",
        "\n",
        "# prompt: write detail Python documentation with default value for the following function: print_aug_ai_sequential()\n",
        "import nlpaug\n",
        "@add_method(PacktDataAug)\n",
        "def print_aug_ai_sequential(self, df,\n",
        "  aug_name=\"T5_summary\",\n",
        "  bsize=4):\n",
        "\n",
        "  \"\"\"\n",
        "  Augment text using a sequence of augmenters.\n",
        "\n",
        "  Args:\n",
        "      df (pd.DataFrame): Input pandas DataFrame.\n",
        "      aug_name (str): Name of the augmentation. Default is 'T5_summary'.\n",
        "      bsize (int): Batch size for augmentation. Default is 4.\n",
        "\n",
        "  Returns:\n",
        "    pd.DataFrame: Augmented pandas DataFrame.\n",
        "  \"\"\"\n",
        "\n",
        "  aug_func = nlpaug.flow.Sequential([self.ai_aug_bert, self.ai_aug_glove])\n",
        "  orig = df[aug_name][0]\n",
        "  self._print_aug_ai(orig, aug_func,bsize=bsize, aug_name=aug_name)\n",
        "  return"
      ],
      "metadata": {
        "id": "aR5dVL-RNGpm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pluto.df_t5_carroll.T5_summary[0]"
      ],
      "metadata": {
        "id": "Ux5lEe40U3Vk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pluto.print_aug_ai_sequential(pluto.df_t5_carroll)"
      ],
      "metadata": {
        "id": "IvOfkZG_SpQC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pluto.print_aug_ai_sequential(pluto.df_t5_dickens)"
      ],
      "metadata": {
        "id": "JraL6RCQWtG8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pluto.print_aug_ai_sequential(pluto.df_t5_melville)"
      ],
      "metadata": {
        "id": "TdqKW0pzWtb9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pluto.print_aug_ai_sequential(pluto.df_t5_self)"
      ],
      "metadata": {
        "id": "kmz35pawWte_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# end of chapter 6\n",
        "print('end of chatper 6')"
      ],
      "metadata": {
        "id": "tzS8W--FEXHD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check on all auto AI generated doc\n",
        "help(pluto)"
      ],
      "metadata": {
        "id": "GL-0OEsI0aNJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Push up all changes (Optional)"
      ],
      "metadata": {
        "id": "-8QoaB1uZj5N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import os\n",
        "# f = 'Data-Augmentation-with-Python'\n",
        "# os.chdir(f)\n",
        "# !git add -A\n",
        "# !git config --global user.email \"duc.haba@gmail.com\"\n",
        "# !git config --global user.name \"duchaba\"\n",
        "# !git commit -m \"end of session\"\n",
        "# # do the git push in the xterm console\n",
        "# #!git push"
      ],
      "metadata": {
        "id": "-uqlOsMlZm5y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "joLMiWRhEV0W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Summary\n",
        "\n",
        "Every chaper will begin with same base class \"PacktDataAug\".\n",
        "\n",
        "âœ‹ FAIR WARNING:\n",
        "\n",
        "- The coding uses long and complete function path name.\n",
        "\n",
        "- Pluto wrote the code for easy to understand and not for compactness, fast execution, nor cleaverness.\n",
        "\n",
        "- Use Xterm to debug cloud server"
      ],
      "metadata": {
        "id": "X1kgd3PWEjKj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install colab-xterm\n",
        "# %load_ext colabxterm\n",
        "# %xterm"
      ],
      "metadata": {
        "id": "3n1wuFxaZm9I"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuClass": "premium",
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "gpuClass": "premium"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}