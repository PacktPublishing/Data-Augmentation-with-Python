{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/duchaba/Data-Augmentation-with-Python/blob/main/data_augmentation_with_python_chapter_5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸŒ» Welcome to Chapter 5, Text Augmentation\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "I am glad to see you using this Python Notebook. ðŸ¶\n",
        "\n",
        "The Python Notebook is an integral part of the book. You can add new â€œcode cellsâ€ to extend the functions, add your data, and explore new possibilities, such as downloading additional real-world datasets from the Kaggle website and coding the **Fun challenges**. Furthermore, the book has **Fun facts**, in-depth discussion about augmentation techniques, and Pluto, an imaginary Siberian Huskey coding companion. Together they will guide you every steps of the way. \n",
        "\n",
        "Pluto encourages you to copy or save a copy of this Python Notebook to your local space and add the â€œtext cellsâ€ to keep your notes. In other words, read the book and copy the relevant concept to this Python Notebookâ€™s text-cells. Thus, you can have the explanation, note, original code, your code, and any crazy future ideas in one place.  \n",
        "\n",
        "\n",
        "ðŸ’— I hope you enjoy reading the book and hacking code as much as I enjoy writing it. \n",
        "\n",
        "\n",
        "## ðŸŒŸ Amazon Book\n",
        "\n",
        "---\n",
        "\n",
        "- The book is available on the Amazon Book website: \n",
        "  - https://www.amazon.com/dp/1803246456\n",
        "\n",
        "  - Author: Duc Haba\n",
        "  - Published: 2023\n",
        "  - Page count: 390+\n",
        "\n",
        "\n",
        "- The original Python Notebook is on: \n",
        "  - https://github.com/PacktPublishing/Data-Augmentation-with-Python/blob/main/Chapter_5/data_augmentation_with_python_chapter_5.ipynb \n",
        "\n",
        "- ðŸš€ Click on the blue \"Open in Colab\" button at the top of this page to begin hacking.\n",
        "\n"
      ],
      "metadata": {
        "id": "rad13-eq4fAm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸ˜€ Excerpt from Chapter 5, Text Augmentation\n",
        "\n",
        "---\n",
        "\n",
        "> In case you havenâ€™t bought the book. Here is an teaser from the first page of Chapter 5.\n",
        "\n",
        "---\n",
        "\n",
        "Text augmentation is a technique that is used in Natural Language Processing (NLP) to generate additional data by modifying or creating new text from existing text data. Text augmentation involves techniques such as character swapping, noise injection, synonym replacement, word deletion, word insertion, and word swapping. Image and text augmentation have the same goal. They strive to increase the size of the training dataset and improve AI prediction accuracy.\n",
        "\n",
        "Text augmentation is relatively more challenging to evaluate because it is not as intuitive as image augmentation. The intent of an image augmentation technique is clear, such as flipping a photo, but a character-swapping technique will be disorienting to the reader. Therefore, readers might perceive the benefits as subjective.\n",
        "\n",
        "The effectiveness of text augmentation depends on the quality of the generated data and the specific NLP task being performed. It can be challenging to determine the appropriate safe level of text augmentation that is required for a given dataset, and it often requires experimentation and testing to achieve the desired results.\n",
        "\n",
        "Customer feedback or social media chatter is fair game for text augmentation because the writing is messy and, predominantly, contains grammatical errors. Conversely, legal documents or written medical communications, such as doctorâ€™s prescriptions or reports, are off-limits because the message is precise. In other words, error injections, synonyms, or even AI-generated text might change the legal or medical meaning beyond the safe level.\n",
        "\n",
        "The biases in text augmentation are equally difficult to discern. For example, adding noise by purposing misspell words using the Keyboard augmentation method might introduce bias against real-world tweets, which typically contain misspelled words. There are no generalized rules to follow, and the answer only becomes evident after thoroughly studying the data and reviewing the AI forecasting objective.\n",
        "\n",
        "---\n",
        "\n",
        "Fun fact\n",
        "\n",
        "---\n",
        "\n",
        "As generative AI becomes more widely available, you can use OpenAIâ€™s GPT-3, Google Bard or Facebookâ€™s Roberta system to generate original articles for text augmentation. For example, you can ask generative AI to create positive or negative reviews about a company product, then use the AI-written articles to train predictive AI on sentiment analysis.\n",
        "\n",
        "---\n",
        "\n",
        "In Chapter 5, you will learn about text augmentation and how to code the methods in Python notebooks. In particular, we will cover the following topics:\n",
        "\n",
        "- Character augmenting\n",
        "\n",
        "- Word augmenting\n",
        "\n",
        "- Sentence and flow augmenting\n",
        "\n",
        "- Text augmentation libraries\n",
        "\n",
        "- Real-world text datasets\n",
        "\n",
        "- Reinforce learning through Python Notebook\n",
        "\n",
        "Letâ€™s get started with the simplest topic, character augmentation.\n",
        "\n",
        "---\n",
        "\n",
        "ðŸŒ´ *end of excerpt from the book*"
      ],
      "metadata": {
        "id": "ZZ8-JIXv6jmV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GitHub Clone"
      ],
      "metadata": {
        "id": "4b52gSlp60SN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# git version should be 2.17.1 or higher\n",
        "!git --version"
      ],
      "metadata": {
        "id": "6oeDAu1u6zWf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "url = 'https://github.com/PacktPublishing/Data-Augmentation-with-Python'\n",
        "!git clone {url}"
      ],
      "metadata": {
        "id": "J9buwUR767Te"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fetch file from URL (Optional)\n",
        "\n",
        "- Uncommend the below 2 code cells if you want to use URL and not Git Clone"
      ],
      "metadata": {
        "id": "lOawA01L7Ok0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import requests\n",
        "# #\n",
        "# def fetch_file(url, dst):\n",
        "#   downloaded_obj = requests.get(url)\n",
        "#   with open(dst, \"wb\") as file:\n",
        "#     file.write(downloaded_obj.content)\n",
        "#   return"
      ],
      "metadata": {
        "id": "HsmQ7rgj67Ww"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# url = ''\n",
        "# dst = 'pluto_chapter_1.py'\n",
        "# fetch_file(url,dst)"
      ],
      "metadata": {
        "id": "_nKp0nxT67aH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run Pluto\n",
        "\n",
        "- Instantiate up Pluto, aka. \"Pluto, wake up!\""
      ],
      "metadata": {
        "id": "6462KTtr7cFQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# %% CARRY-OVER code install\n",
        "\n",
        "!pip install opendatasets --upgrade\n",
        "!pip install pyspellchecker "
      ],
      "metadata": {
        "id": "jCGiTw16ZxVH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#load and run the pluto chapter 1 Python code.\n",
        "pluto_file = 'Data-Augmentation-with-Python/pluto/pluto_chapter_2.py'\n",
        "%run {pluto_file}"
      ],
      "metadata": {
        "id": "u3zbkOO86_WB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Verify Pluto"
      ],
      "metadata": {
        "id": "yhRR0JSf746h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pluto.say_sys_info()"
      ],
      "metadata": {
        "id": "nBIw7fiI6_Zy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## (Optional) Export to .py"
      ],
      "metadata": {
        "id": "LaURlsasaGuo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pluto_chapter_5 = 'Data-Augmentation-with-Python/pluto/pluto_chapter_5.py'\n",
        "!cp {pluto_file} {pluto_chapter_5}"
      ],
      "metadata": {
        "id": "Gfx9Ai5Opyqp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# âœ‹ Set up Kaggle username and app Key\n",
        "\n",
        "- Install the following libraries, and import it on the Notebook.\n",
        "- Follow by initialize Kaggle username, key and fetch methods.\n",
        "- STOP: Update your Kaggle access username or key first."
      ],
      "metadata": {
        "id": "kQ6ap39x8HyF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# %%CARRY-OVER code \n",
        "\n",
        "# -------------------- : --------------------\n",
        "# READ ME\n",
        "# Chapter 2 begin:\n",
        "# Install the following libraries, and import it on the Notebook.\n",
        "# Follow by initialize Kaggle username, key and fetch methods.\n",
        "# STOP: Update your Kaggle access username or key first.\n",
        "# -------------------- : --------------------\n",
        "\n",
        "!pip install opendatasets --upgrade\n",
        "import opendatasets\n",
        "print(\"\\nrequired version 0.1.22 or higher: \", opendatasets.__version__)\n",
        "\n",
        "!pip install pyspellchecker \n",
        "import spellchecker\n",
        "print(\"\\nRequired version 0.7+\", spellchecker.__version__)\n",
        "\n",
        "# STOP: Update your Kaggle access username or key first.\n",
        "pluto.remember_kaggle_access_keys(\"YOUR_KAGGLE_USERNAME\", \"YOUR_KAGGLE_API_KEY\")\n",
        "pluto._write_kaggle_credit()\n",
        "import kaggle\n",
        "\n",
        "@add_method(PacktDataAug)\n",
        "def fetch_kaggle_comp_data(self,cname):\n",
        "  #self._write_kaggle_credit()  # need to run only once.\n",
        "  path = pathlib.Path(cname)\n",
        "  kaggle.api.competition_download_cli(str(path))\n",
        "  zipfile.ZipFile(f'{path}.zip').extractall(path)\n",
        "  return\n",
        "\n",
        "@add_method(PacktDataAug)\n",
        "def fetch_kaggle_dataset(self,url,dest=\"kaggle\"):\n",
        "  #self._write_kaggle_credit()    # need to run only once.\n",
        "  opendatasets.download(url,data_dir=dest)\n",
        "  return\n",
        "# -------------------- : --------------------\n"
      ],
      "metadata": {
        "id": "sf4Obdp-77CL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fetch Kaggle Data"
      ],
      "metadata": {
        "id": "5BfHiMQE8XWp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## NetFlix"
      ],
      "metadata": {
        "id": "WSWTfhPU9YjQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "url = 'https://www.kaggle.com/datasets/infamouscoder/dataset-netflix-shows'\n",
        "pluto.fetch_kaggle_dataset(url)"
      ],
      "metadata": {
        "id": "_TEFhsYs77Go"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f = 'kaggle/dataset-netflix-shows/netflix_titles.csv'\n",
        "pluto.df_netflix_data = pluto.fetch_df(f)\n",
        "pluto.df_netflix_data.head(3)"
      ],
      "metadata": {
        "id": "_GHi9DnM77Kj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pluto.print_batch_text(pluto.df_netflix_data)"
      ],
      "metadata": {
        "id": "FJP18_-M77Nw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pluto.count_word(pluto.df_netflix_data)\n",
        "pluto.df_netflix_data.head()"
      ],
      "metadata": {
        "id": "EC_P1qsP77RG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pluto.draw_word_count(pluto.df_netflix_data)"
      ],
      "metadata": {
        "id": "HTXz32vF6_cn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %%CARRY-OVER code install\n",
        "\n",
        "!pip install missingno"
      ],
      "metadata": {
        "id": "QAxo1zXThdkM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %%writefile -a {pluto_chapter_5}\n",
        "\n",
        "pluto.version = 5.0\n",
        "import missingno\n",
        "@add_method(PacktDataAug)\n",
        "def draw_text_null_data(self, df, color=(0.3,0.36,0.44)):\n",
        "  canvas, pic = matplotlib.pyplot.subplots(1, 1, figsize=(10, 6))\n",
        "  missingno.matrix(df,color=color,ax=pic)\n",
        "  pic.set_title('Missing Data (Null Value)')\n",
        "  pic.set_xlabel('Solid is has data. White line is missing/null data.')\n",
        "  canvas.tight_layout()\n",
        "  self._drop_image(canvas)\n",
        "  canvas.show()\n",
        "  return"
      ],
      "metadata": {
        "id": "uN3KPZBbhadh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pluto.draw_text_null_data(pluto.df_netflix_data)"
      ],
      "metadata": {
        "id": "-MaTeOMghuR2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %%CARRY-OVER code install\n",
        "\n",
        "!pip install nltk\n",
        "!pip install wordcloud\n",
        "#\n",
        "# tested with the following version\n",
        "# !pip install nltk==3.8.1\n",
        "# !pip install wordcloud==1.8.2.2"
      ],
      "metadata": {
        "id": "oBGVKXYGh_z-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %%writefile -a {pluto_chapter_5}\n",
        "\n",
        "import nltk\n",
        "import wordcloud\n",
        "import re\n",
        "@add_method(PacktDataAug)\n",
        "def _draw_image_wordcloud(self, words_str, xignore_words='cat', title='Word Cloud:'):\n",
        "  canvas, pic = matplotlib.pyplot.subplots(1, 1, figsize=(16, 8))\n",
        "  img = wordcloud.WordCloud(width = 1400, \n",
        "    height = 800, \n",
        "    background_color ='white',\n",
        "    stopwords = xignore_words, \n",
        "    min_font_size = 10).generate(words_str) \n",
        "  pic.imshow(img)\n",
        "  pic.set_title(title)\n",
        "  pic.set_xlabel(f'Approximate Words: {int(len(words_str) / 5)}')\n",
        "  pic.tick_params(left = False, right = False, labelleft = False,\n",
        "    labelbottom = False, bottom = False)\n",
        "  canvas.tight_layout()\n",
        "  self._drop_image(canvas)\n",
        "  canvas.show()\n",
        "  return\n",
        "  #\n",
        "@add_method(PacktDataAug)\n",
        "def draw_text_wordcloud(self, df_1column, xignore_words='cat', title='Word Cloud:'):\n",
        "  orig = df_1column.str.cat()\n",
        "  clean = re.sub('[^A-Za-z0-9 ]+', '', orig)\n",
        "  self._draw_image_wordcloud(clean, xignore_words=xignore_words,title=title)\n",
        "  return"
      ],
      "metadata": {
        "id": "f2Y799hsv7Kf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Nltk version 3.7: actual: ', nltk.__version__)\n",
        "print('WordCloud version 1.8.2.2: actual: ', wordcloud.__version__)"
      ],
      "metadata": {
        "id": "bSauNuC1wGzJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pluto.draw_text_wordcloud(pluto.df_netflix_data.description, \n",
        "  xignore_words=wordcloud.STOPWORDS, \n",
        "  title='Word Cloud: Netflix Movie Review')"
      ],
      "metadata": {
        "id": "OaGD2wtPANLR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Twitter"
      ],
      "metadata": {
        "id": "ZurnPA1u-T8z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @add_method(PacktDataAug)\n",
        "# def fetch_df(self, csv):\n",
        "#   df = pandas.read_csv(csv, encoding='latin-1')\n",
        "#   return df"
      ],
      "metadata": {
        "id": "FRjmqGVn-msp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# \n",
        "%%time\n",
        "url = 'https://www.kaggle.com/datasets/mayurdalvi/twitter-sentiments-analysis-nlp'\n",
        "pluto.fetch_kaggle_dataset(url)"
      ],
      "metadata": {
        "id": "WePFW0tu6_fe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# remove white space in directory and filename\n",
        "# run this until no error/output\n",
        "f = 'kaggle/twitter-sentiments-analysis-nlp'\n",
        "#!find {f} -name \"* *\" -type d | rename 's/ /_/g'\n",
        "!find {f} -name \"* *\" -type f | rename 's/ /_/g'"
      ],
      "metadata": {
        "id": "ms7jgEcrKpIr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f = 'kaggle/twitter-sentiments-analysis-nlp/Twitter_Sentiments.csv'\n",
        "pluto.df_twitter_data = pluto.fetch_df(f)\n",
        "pluto.df_twitter_data.head(3)"
      ],
      "metadata": {
        "id": "HhaenARf-mmJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pluto.print_batch_text(pluto.df_twitter_data,cols=['label', 'tweet'])"
      ],
      "metadata": {
        "id": "onbBfDI--mvl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %%CARRY-OVER code install\n",
        "\n",
        "!pip install filter-profanity\n",
        "#\n",
        "# tested on the following on version:\n",
        "#!pip install filter-profanity==1.0.9"
      ],
      "metadata": {
        "id": "E7s_6RTlBXJr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %%writefile -a {pluto_chapter_5}\n",
        "\n",
        "import profanity\n",
        "import re\n",
        "#\n",
        "@add_method(PacktDataAug)\n",
        "def _clean_text(self,x):\n",
        "  return (re.sub('[^A-Za-z0-9 .,!?#@]+', '', str(x)))\n",
        "#\n",
        "@add_method(PacktDataAug)\n",
        "def _clean_bad_word(self,x):\n",
        "  return (profanity.censor_profanity(x, ''))\n",
        "#\n",
        "@add_method(PacktDataAug)\n",
        "def clean_text(self, df):\n",
        "  df['clean_tweet'] = df.tweet.apply(self._clean_text)\n",
        "  df['clean_tweet'] = df['clean_tweet'].apply(self._clean_bad_word)\n",
        "  return df"
      ],
      "metadata": {
        "id": "R7NuXRci5wC9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "pluto.clean_text(pluto.df_twitter_data)\n",
        "pluto.df_twitter_data.head()\n",
        "#\n",
        "pluto.df_netflix_data['description'] = pluto.df_netflix_data['description'].apply(pluto._clean_text)"
      ],
      "metadata": {
        "id": "RDqqtRY883yk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pluto.print_batch_text(pluto.df_twitter_data, cols=['label', 'clean_tweet'])"
      ],
      "metadata": {
        "id": "ViaQTmnBc-0U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# double check on clean tweets\n",
        "print('clean: ', pluto.df_twitter_data.clean_tweet[13538], ' : original: ', \n",
        "  pluto.df_twitter_data.tweet[13538], ': label: ', pluto.df_twitter_data.label[13538])"
      ],
      "metadata": {
        "id": "3cVkaqIveWwU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# double check on clean tweets\n",
        "with pandas.option_context(\"display.max_colwidth\", None):\n",
        "  display(pluto.df_twitter_data[pluto.df_twitter_data.label == 1].sample(10))"
      ],
      "metadata": {
        "id": "8uKcbMAefmfN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pluto.count_word(pluto.df_twitter_data,col_dest='clean_tweet')\n",
        "pluto.draw_word_count(pluto.df_twitter_data)"
      ],
      "metadata": {
        "id": "wsyzuXJ_-my0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pluto.draw_text_null_data(pluto.df_twitter_data)"
      ],
      "metadata": {
        "id": "pzqZtxaIh2lw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pluto.draw_text_wordcloud(pluto.df_twitter_data.clean_tweet,\n",
        "  xignore_words=wordcloud.STOPWORDS,\n",
        "  title='Clean Tweets Word Cloud')"
      ],
      "metadata": {
        "id": "QBGppWuskKZf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Export (aka drop or save) data file"
      ],
      "metadata": {
        "id": "iubboKaOM5-Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# %%writefile -a {pluto_chapter_5}\n",
        "\n",
        "@add_method(PacktDataAug)\n",
        "def _drop_df_file(self, df,fname,type='csv',sep='~'):\n",
        "  df.to_csv(fname,sep=sep)\n",
        "  return "
      ],
      "metadata": {
        "id": "oVO43N-Dh2pL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f = 'Data-Augmentation-with-Python/pluto_data/netflix_data.csv'\n",
        "pluto._drop_df_file(pluto.df_netflix_data, f)"
      ],
      "metadata": {
        "id": "FDswusz3LTfK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f = 'Data-Augmentation-with-Python/pluto_data/twitter_data.csv'\n",
        "pluto._drop_df_file(pluto.df_twitter_data, f)"
      ],
      "metadata": {
        "id": "EmYEusRiMkSD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oZqCjjxuOvZN"
      },
      "source": [
        "# Character Augmenter<a class=\"anchor\" id=\"chara_aug\">\n",
        "\n",
        "Augmenting data in character level. Possible scenarios include image to text and chatbot. During recognizing text from image, we need to optical character recognition (OCR) model to achieve it but OCR introduces some errors such as recognizing \"o\" and \"0\". `OCRAug` simulate these errors to perform the data augmentation. For chatbot, we still have typo even though most of application comes with word correction. Therefore, `KeyboardAug` is introduced to simulate this kind of errors."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %%CARRY-OVER code install\n",
        "\n",
        "!pip install nlpaug\n",
        "#\n",
        "# tested on the following version:\n",
        "#!pip install nlpaug==1.1.11"
      ],
      "metadata": {
        "id": "rtHdmFVaPBtC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %%writefile -a {pluto_chapter_5}\n",
        "\n",
        "import nlpaug\n",
        "import nlpaug.augmenter\n",
        "import nlpaug.augmenter.char\n",
        "import nlpaug.augmenter.word"
      ],
      "metadata": {
        "id": "4gs85bUcGTXx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('version 1.1.11, actual: ',nlpaug.__version__)"
      ],
      "metadata": {
        "id": "TS8IO9pWo40i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %%writefile -a {pluto_chapter_5}\n",
        "\n",
        "pluto.orig_text = 'It was the best of times. It was the worst of times. It was the age of wisdom. It was the age of foolishness. It was the epoch of belief. It was the epoch of incredulity.'"
      ],
      "metadata": {
        "id": "6nl9A5n1G_WS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %%writefile -a {pluto_chapter_5}\n",
        "\n",
        "@add_method(PacktDataAug)\n",
        "def _print_aug_batch(self, df, aug_func, col_dest=\"description\",\n",
        "  bsize=3, aug_name='Augmented',is_larger_font=True):\n",
        "  col_name = [aug_name, 'Original']\n",
        "  aug = aug_func.augment(self.orig_text, n=1)\n",
        "  data = [[aug[0], self.orig_text]]\n",
        "  df_aug = pandas.DataFrame(data, columns=col_name)\n",
        "  orig = df[col_dest].sample(bsize)\n",
        "  for tx in orig:\n",
        "    aug = aug_func.augment(tx, n=1)\n",
        "    data = [[aug[0], tx]]\n",
        "    t = pandas.DataFrame(data, columns=col_name)\n",
        "    df_aug = df_aug.append(t, ignore_index=True)\n",
        "  #\n",
        "  with pandas.option_context(\"display.max_colwidth\", None):\n",
        "    if (is_larger_font):\n",
        "      display(df_aug.head(bsize+1).style.set_table_styles(self._fetch_larger_font()))\n",
        "    else:\n",
        "      display(df_aug.head(bsize+1))\n",
        "  return"
      ],
      "metadata": {
        "id": "_OhmyNQRHIfS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## OCR augmenting"
      ],
      "metadata": {
        "id": "sAX4XyY6bpW3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# %%writefile -a {pluto_chapter_5}\n",
        "\n",
        "@add_method(PacktDataAug)\n",
        "def print_aug_ocr(self, df, col_dest=\"description\",bsize=3, aug_name='Augmented'):\n",
        "  aug_func = nlpaug.augmenter.char.OcrAug()\n",
        "  self._print_aug_batch(df, aug_func,col_dest=col_dest,bsize=bsize, aug_name=aug_name)\n",
        "  return"
      ],
      "metadata": {
        "id": "RUz21pdqYv3i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pluto.print_aug_ocr(pluto.df_netflix_data, col_dest='description',aug_name='OCR Augment')"
      ],
      "metadata": {
        "id": "sb_tMYrxY9nV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pluto.print_aug_ocr(pluto.df_twitter_data, col_dest='clean_tweet',aug_name='OCR Augment')"
      ],
      "metadata": {
        "id": "NtOYQz1FY9qy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uHDd-yZcOvZQ"
      },
      "source": [
        "## Keyboard Augmenter<a class=\"anchor\" id=\"keyboard_aug\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bOg0vDdbOvZQ"
      },
      "source": [
        "- Substitute character by keyboard distance"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %%writefile -a {pluto_chapter_5}\n",
        "\n",
        "@add_method(PacktDataAug)\n",
        "def print_aug_keyboard(self, df, col_dest=\"description\",bsize=3, aug_name='Keyboard Augment'):\n",
        "  aug_func = nlpaug.augmenter.char.KeyboardAug()\n",
        "  self._print_aug_batch(df, aug_func,col_dest=col_dest,bsize=bsize, aug_name=aug_name)\n",
        "  return"
      ],
      "metadata": {
        "id": "YlB5M1oPeDpe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pluto.print_aug_keyboard(pluto.df_netflix_data, col_dest='description',\n",
        "  aug_name='Keyboard Augment')"
      ],
      "metadata": {
        "id": "hWOa5vpweVKl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pluto.print_aug_keyboard(pluto.df_twitter_data, col_dest='clean_tweet',\n",
        "  aug_name='Keyboard Augment')"
      ],
      "metadata": {
        "id": "0_nCXoCAeVN0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07pLIh0ZOvZR"
      },
      "source": [
        "## Random Augmenter<a class=\"anchor\" id=\"random_aug\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %%writefile -a {pluto_chapter_5}\n",
        "\n",
        "@add_method(PacktDataAug)\n",
        "def print_aug_char_random(self, df, action='insert', col_dest=\"description\",bsize=3, aug_name='Augment'):\n",
        "  aug_func = nlpaug.augmenter.char.RandomCharAug(action=action)\n",
        "  self._print_aug_batch(df, aug_func,col_dest=col_dest,bsize=bsize, aug_name=aug_name)\n",
        "  return"
      ],
      "metadata": {
        "id": "3yEFMY17guY_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pluto.print_aug_char_random(pluto.df_netflix_data, action='insert', \n",
        "  col_dest='description', aug_name='Random Insert Augment')"
      ],
      "metadata": {
        "id": "yeGr1b7qTaoB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pluto.print_aug_char_random(pluto.df_netflix_data, action='delete', \n",
        "  col_dest='description', aug_name='Random Delete Augment')"
      ],
      "metadata": {
        "id": "IiXwaqp2Ta0e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pluto.print_aug_char_random(pluto.df_netflix_data, action='substitute', \n",
        "  col_dest='description', aug_name='Random Substitute Augment')"
      ],
      "metadata": {
        "id": "J9oLzmMqUDTt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pluto.print_aug_char_random(pluto.df_netflix_data, action='swap', \n",
        "  col_dest='description', aug_name='Random Swap Augment')"
      ],
      "metadata": {
        "id": "wMnc0_4tUDXL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pluto.print_aug_char_random(pluto.df_twitter_data, action='insert', \n",
        "  col_dest='clean_tweet', aug_name='Random Insert Augment')"
      ],
      "metadata": {
        "id": "vmemvezFUDaP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pluto.print_aug_char_random(pluto.df_twitter_data, action='delete', \n",
        "  col_dest='clean_tweet', aug_name='Random Delete Augment')"
      ],
      "metadata": {
        "id": "AtuG0YkdUDdp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pluto.print_aug_char_random(pluto.df_twitter_data, action='substitute', \n",
        "  col_dest='clean_tweet', aug_name='Random Substitute Augment')"
      ],
      "metadata": {
        "id": "nLM2luSNUDgW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pluto.print_aug_char_random(pluto.df_twitter_data, action='swap', \n",
        "  col_dest='clean_tweet', aug_name='Random Swap Augment')"
      ],
      "metadata": {
        "id": "dXQmgum3UDjH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Word augmentation"
      ],
      "metadata": {
        "id": "YH7uYnPvzaWn"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YfCX_OBFOvZV"
      },
      "source": [
        "## Misspell Augmenter<a class=\"anchor\" id=\"spelling_aug\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "98-Qs4wUOvZV"
      },
      "source": [
        "- Substitute word by spelling mistake words dictionary"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %%writefile -a {pluto_chapter_5}\n",
        "\n",
        "@add_method(PacktDataAug)\n",
        "def print_aug_word_misspell(self, df, col_dest=\"description\",bsize=3, aug_name='Augment'):\n",
        "  aug_func = nlpaug.augmenter.word.SpellingAug()\n",
        "  self._print_aug_batch(df, aug_func,col_dest=col_dest,bsize=bsize, aug_name=aug_name)\n",
        "  return"
      ],
      "metadata": {
        "id": "cKXwiAXgWO8z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pluto.print_aug_word_misspell(pluto.df_netflix_data, \n",
        "  col_dest='description', aug_name='Word Spelling Augment')"
      ],
      "metadata": {
        "id": "ha0e6lcdXVPs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pluto.print_aug_word_misspell(pluto.df_twitter_data, \n",
        "  col_dest='clean_tweet', aug_name='Word Spelling Augment')"
      ],
      "metadata": {
        "id": "ohHaLHRHXvtL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AcPTut6FOvZi"
      },
      "source": [
        "## Split Augmenter<a class=\"anchor\" id=\"split_aug\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iZJhHv9HOvZi"
      },
      "source": [
        "- Split word to two tokens randomly"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %%writefile -a {pluto_chapter_5}\n",
        "\n",
        "@add_method(PacktDataAug)\n",
        "def print_aug_word_split(self, df, col_dest=\"description\",bsize=3, aug_name='Augment'):\n",
        "  aug_func = nlpaug.augmenter.word.SplitAug()\n",
        "  self._print_aug_batch(df, aug_func,col_dest=col_dest,bsize=bsize, aug_name=aug_name)\n",
        "  return"
      ],
      "metadata": {
        "id": "2D0ybu_1YDog"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pluto.print_aug_word_split(pluto.df_netflix_data, col_dest='description', aug_name='Word Split Augment')"
      ],
      "metadata": {
        "id": "IML8dabSYRc7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pluto.print_aug_word_split(pluto.df_twitter_data, col_dest='clean_tweet', aug_name='Word Split Augment')"
      ],
      "metadata": {
        "id": "lTfSH_rPYRgW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GOjTDye8OvZf"
      },
      "source": [
        "## Random Word Augmenter"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %%writefile -a {pluto_chapter_5}\n",
        "\n",
        "@add_method(PacktDataAug)\n",
        "def print_aug_word_random(self, df, action='swap', col_dest=\"description\",bsize=3, aug_name='Augment'):\n",
        "  aug_func = nlpaug.augmenter.word.RandomWordAug(action=action)\n",
        "  self._print_aug_batch(df, aug_func,col_dest=col_dest,bsize=bsize, aug_name=aug_name)\n",
        "  return"
      ],
      "metadata": {
        "id": "OP0EgFV1ZS4n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pluto.print_aug_word_random(pluto.df_netflix_data, action='swap', \n",
        "  col_dest='description', aug_name='Word Random Swap Augment')"
      ],
      "metadata": {
        "id": "t0_1pjpuaX9C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pluto.print_aug_word_random(pluto.df_twitter_data, action='swap', \n",
        "  col_dest='clean_tweet', aug_name='Word Random Swap Augment')"
      ],
      "metadata": {
        "id": "6kFTA5XkaYK6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pluto.print_aug_word_random(pluto.df_netflix_data, action='substitute', \n",
        "  col_dest='description', aug_name='Word Random Substitude Augment')"
      ],
      "metadata": {
        "id": "ux703ByVaYA7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pluto.print_aug_word_random(pluto.df_twitter_data, action='substitute', \n",
        "  col_dest='clean_tweet', aug_name='Word Random Substitute Augment')"
      ],
      "metadata": {
        "id": "MLWj8WKzaYOz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pluto.print_aug_word_random(pluto.df_netflix_data, action='crop', \n",
        "  col_dest='description', aug_name='Word Random Crop Augment')"
      ],
      "metadata": {
        "id": "Fz7grRQYaYFf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pluto.print_aug_word_random(pluto.df_twitter_data, action='crop', \n",
        "  col_dest='clean_tweet', aug_name='Word Random Crop Augment')"
      ],
      "metadata": {
        "id": "g493xRrDaYSP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pluto.print_aug_word_random(pluto.df_netflix_data, action='delete', \n",
        "  col_dest='description', aug_name='Word Random Delete Augment')"
      ],
      "metadata": {
        "id": "jLfWfnT5aYH0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pluto.print_aug_word_random(pluto.df_twitter_data, action='delete', \n",
        "  col_dest='clean_tweet', aug_name='Word Random Delete Augment')"
      ],
      "metadata": {
        "id": "nETKRYFTaYVm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HjmXSJndOvZd"
      },
      "source": [
        "## Synonym Augmenter"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %%writefile -a {pluto_chapter_5}\n",
        "\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "83ODKdEYPowa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yOImDH4QOvZd"
      },
      "source": [
        "- Substitute word by WordNet's synonym"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %%writefile -a {pluto_chapter_5}\n",
        "\n",
        "@add_method(PacktDataAug)\n",
        "def print_aug_word_synonym(self, df, col_dest=\"description\",bsize=3, aug_name='Augment'):\n",
        "  aug_func = nlpaug.augmenter.word.SynonymAug(aug_src='wordnet')\n",
        "  self._print_aug_batch(df, aug_func,col_dest=col_dest,bsize=bsize, aug_name=aug_name)\n",
        "  return"
      ],
      "metadata": {
        "id": "QrJnm8k1dXAR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pluto.print_aug_word_synonym(pluto.df_netflix_data, \n",
        "  col_dest='description', aug_name='Synonym WordNet Augment')"
      ],
      "metadata": {
        "id": "y075AG5jdand"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pluto.print_aug_word_synonym(pluto.df_twitter_data, \n",
        "  col_dest='clean_tweet', aug_name='Synonym WordNet Augment')"
      ],
      "metadata": {
        "id": "zOcCjlNkdaq1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XY7cu6wUOvZe"
      },
      "source": [
        "## Antonym Augmenter<a class=\"anchor\" id=\"antonym_aug\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %%writefile -a {pluto_chapter_5}\n",
        "\n",
        "@add_method(PacktDataAug)\n",
        "def print_aug_word_antonym(self, df, col_dest=\"description\",bsize=3, aug_name='Augment'):\n",
        "  aug_func = nlpaug.augmenter.word.AntonymAug()\n",
        "  self._print_aug_batch(df, aug_func,col_dest=col_dest,bsize=bsize, aug_name=aug_name)\n",
        "  return"
      ],
      "metadata": {
        "id": "PvF8q32eeysk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pluto.print_aug_word_antonym(pluto.df_netflix_data, \n",
        "  col_dest='description',aug_name='Antonym Augment')"
      ],
      "metadata": {
        "id": "7tqxTK-1eyvw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pluto.print_aug_word_antonym(pluto.df_twitter_data, \n",
        "  col_dest='clean_tweet',aug_name='Antonym Augment')"
      ],
      "metadata": {
        "id": "FGJEo3vHeyzB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kC0Vb6x9OvZk"
      },
      "source": [
        "## Reserved Word Augmenter<a class=\"anchor\" id=\"reserved_aug\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %%writefile -a {pluto_chapter_5}\n",
        "\n",
        "@add_method(PacktDataAug)\n",
        "def print_aug_word_reserved(self, df, col_dest=\"description\",reserved_tokens=None,bsize=3, aug_name='Augment'):\n",
        "  aug_func = nlpaug.augmenter.word.ReservedAug(reserved_tokens=reserved_tokens)\n",
        "  self._print_aug_batch(df, aug_func,col_dest=col_dest,bsize=bsize, aug_name=aug_name)\n",
        "  return"
      ],
      "metadata": {
        "id": "Vl927YB886BM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %%writefile -a {pluto_chapter_5}\n",
        "\n",
        "pluto.reserved_control = [['wisdom', 'sagacity', 'intelligence', 'prudence'],\n",
        "  ['foolishness', 'folly', 'idiocy', 'stupidity']]"
      ],
      "metadata": {
        "id": "LasXSvw186OP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %%writefile -a {pluto_chapter_5}\n",
        "\n",
        "pluto.reserved_netflix = [['family','household', 'brood', 'unit', 'families'],\n",
        "  ['life','existance', 'entity', 'creation'],\n",
        "  ['love', 'warmth', 'endearment','tenderness']]\n",
        "pluto.reserved_netflix = pluto.reserved_control + pluto.reserved_netflix"
      ],
      "metadata": {
        "id": "Ilef_J_987PV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pluto.print_aug_word_reserved(pluto.df_netflix_data, col_dest='description', \n",
        "  reserved_tokens=pluto.reserved_netflix, aug_name='Netflix Reserved word augment')"
      ],
      "metadata": {
        "id": "bY2PVusC_ad7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %%writefile -a {pluto_chapter_5}\n",
        "\n",
        "pluto.reserved_twitter = [['user', 'users', 'customer', 'client','people','member','shopper'],\n",
        "  ['happy', 'cheerful', 'joyful', 'carefree'],\n",
        "  ['time','clock','hour']]\n",
        "pluto.reserved_twitter = pluto.reserved_control + pluto.reserved_twitter"
      ],
      "metadata": {
        "id": "y6ugzuz2_ahT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pluto.reserved_twitter"
      ],
      "metadata": {
        "id": "9n-tJ8qqD1rt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pluto.print_aug_word_reserved(pluto.df_twitter_data, col_dest='clean_tweet', \n",
        "  reserved_tokens=pluto.reserved_twitter,aug_name='Twitter Reserved word augment')"
      ],
      "metadata": {
        "id": "oJz48SZ9D1vC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# end of chapter 5\n",
        "print('End of chapter 5.')"
      ],
      "metadata": {
        "id": "ByK2v2hxOPLM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Push up all changes (Optional)"
      ],
      "metadata": {
        "id": "LEj7fgaIN9_S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- username: duchaba\n",
        "\n",
        "- password: [use the token]"
      ],
      "metadata": {
        "id": "lHXRf21BT9N8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import os\n",
        "# f = 'Data-Augmentation-with-Python'\n",
        "# os.chdir(f)\n",
        "# !git add -A\n",
        "# !git config --global user.email \"duc.haba@gmail.com\"\n",
        "# !git config --global user.name \"duchaba\"\n",
        "# !git commit -m \"end of session\"\n",
        "# # do the git push in the xterm console\n",
        "# #!git push"
      ],
      "metadata": {
        "id": "eSXJKJFlOEeE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %%script false --no-raise-error  #temporary stop execute for export file"
      ],
      "metadata": {
        "id": "JMJgBjC1Px5A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EzO7lDYDWeLz"
      },
      "source": [
        "# Summary "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X2iUPf3EWePd"
      },
      "source": [
        "Every chaper will begin with same base class \"PacktDataAug\".\n",
        "\n",
        "âœ‹ FAIR WARNING:\n",
        "\n",
        "- The coding uses long and complete function path name.\n",
        "\n",
        "- Pluto wrote the code for easy to understand and not for compactness, fast execution, nor cleaverness.\n",
        "\n",
        "- Use Xterm to debug cloud server\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install colab-xterm\n",
        "# %load_ext colabxterm\n",
        "# %xterm"
      ],
      "metadata": {
        "id": "vXRG3KaeWbGx"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "nlpaug_dev",
      "language": "python",
      "name": "nlpaug_dev"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}